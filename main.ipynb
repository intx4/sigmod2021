{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "from pyspark import SparkContext, SparkConf\n",
    "import tensorflow_hub as hub\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql import functions as f\n",
    "from pyspark.sql import types as t\n",
    "from pyspark.sql import Window as w\n",
    "from graphframes import GraphFrame\n",
    "from pyspark.ml.linalg import DenseVector, SparseVector\n",
    "from pyspark.ml.feature import HashingTF, IDF, Tokenizer, RegexTokenizer, CountVectorizer, StopWordsRemover, NGram, Normalizer, VectorAssembler, Word2Vec, Word2VecModel, PCA\n",
    "from pyspark.ml import Pipeline, Transformer\n",
    "from pyspark.ml.clustering import LDA\n",
    "from pyspark.mllib.linalg import Vectors, VectorUDT\n",
    "conf = (SparkConf().setMaster('local').setAppName('local-1616888250368').set('spark.executor.memory', '15g').set('spark.driver.memory', '20g')\\\n",
    "        .set('spark.sql.broadcastTimeout', '1000'))\n",
    "sc = SparkContext(conf=conf)\n",
    "spark = SparkSession.builder.getOrCreate()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "# 0. Load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "df = spark.read.csv(\"data/X2.csv\", header=True)\n",
    "df.toPandas()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "# 0. Data cleaning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "for c in df.columns:\n",
    "#set everything to lowercase\n",
    "    df = df.withColumn(c, f.lower(f.col(c)))\n",
    "\n",
    "#extract brand or infer from title\n",
    "df = df.drop('ssd_capacity')\n",
    "df = df.withColumn('brand', f.regexp_extract('brand', \"^(\\w+)\", 0))\n",
    "computer_brands = ['(lenovo', 'acer', 'hp', 'dell', 'asus', 'samsung', 'huawei', 'surface', 'apple)']\n",
    "computer_brands_pattern = '|'.join(computer_brands)\n",
    "df = df.withColumn('brand', f.when( f.regexp_extract('title', computer_brands_pattern, 0)!='', f.regexp_extract('title', computer_brands_pattern, 0))\\\n",
    "                   .otherwise(df.brand))\n",
    "#exctract cpu_brand and infer type if intel\n",
    "cpu_brands = ['(intel', 'apple', 'amd', 'nvidia', 'arm)']\n",
    "cpu_pattern = '|'.join(cpu_brands)\n",
    "df = df.withColumn('cpu_model',f.regexp_extract('cpu_model', '(i\\d|pentium|celeron|a\\d)', 0))\n",
    "df = df.withColumn('cpu_model', f.when( (f.regexp_extract('cpu_brand','(intel|amd)', 0 )!='') & f.isnull(df.cpu_model) ,\\\n",
    "                                        f.regexp_extract('cpu_brand', '(i\\d|pentium|celeron|a\\d)', 0))\\\n",
    "                   .otherwise(df.cpu_model))\n",
    "df = df.withColumn('cpu_brand', f.when(f.regexp_extract('cpu_brand', cpu_pattern, 0) != '', f.regexp_extract('cpu_brand', cpu_pattern, 1))\\\n",
    "                                       .otherwise(f.regexp_extract('title', cpu_pattern, 0)))\n",
    "df = df.withColumn('weight', f.when(df.weight.contains('pounds') | df.weight.contains('lbs'),\n",
    "                                    (f.regexp_extract('weight', '(\\d+.?\\d)', 0).cast(t.DoubleType()))).otherwise(\n",
    "                                    f.round(f.regexp_extract('weight', '(\\d+.?\\d)', 0).cast(t.DoubleType())*2.20462,1)\n",
    "                        )\n",
    "                    )\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Blocking\n",
    "Blocking will be done feeding a TF-IDF matrix to an LDA model and extracting\n",
    "keywords from the title matching them to topics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "\"\"\"UTILITIES\"\"\"\n",
    "\n",
    "\"\"\"Returns the df with tokenized columns with stopwords removed\"\"\"\n",
    "def tokenize(df, string_cols):\n",
    "  output = df\n",
    "  stopW = ['softwarecity', 'amazon', 'com','pc', 'windows', 'computers', 'computer', 'accessories', 'laptop', 'notebook', 'kg', 'inch', 'processor', 'memory','gb', 'ram', 'hdd', 'ssd', 'cpu', 'display', 'hz', 'ghz', 'tb','rpm', 'slot', 'slots', 'mhz', 'cache', 'ram', 'ddram', 'dram', 'hd']\n",
    "  for c in string_cols:\n",
    "    output = output.withColumn('temp', f.coalesce(f.col(c), f.lower(c), f.lit('')))\n",
    "    tokenizer = RegexTokenizer(inputCol='temp', outputCol=c+\"_tokens\", pattern = \"\\\\W\")\n",
    "    remover = StopWordsRemover(inputCol=c+\"_tokens\", outputCol=c+\"_swRemoved\", stopWords=stopW)\n",
    "    output = tokenizer.transform(output)\n",
    "\n",
    "    filter_alnum = f.udf(lambda l : [t for t in l if t.isalpha() and len(t) >= 2], t.ArrayType(t.StringType()))\n",
    "    output = output.withColumn(c+'_tokens', filter_alnum(f.col(c+\"_tokens\")))\n",
    "\n",
    "    output = remover.transform(output)\\\n",
    "      .drop('temp', c+\"_tokens\")\n",
    "    # output has c+swRemoved columns\n",
    "  return output\n",
    "\n",
    "def generate_blocking_keys(df, token_cols, min_freq=1):\n",
    "    \"\"\"Pipeline:\n",
    "            1 - CountVectorizer -> TF\n",
    "            2 - IDF\n",
    "            3 - LDA\n",
    "    \"\"\"\n",
    "    df = df.withColumn('tokens_swRemoved', f.concat(*token_cols))\n",
    "    cv = CountVectorizer(inputCol='tokens_swRemoved', outputCol=\"rawFeatures\")\n",
    "    cvmodel = cv.fit(df)\n",
    "    df_vect = cvmodel.transform(df)\n",
    "\n",
    "    idf = IDF(inputCol=\"rawFeatures\", outputCol=\"features\", minDocFreq=min_freq)\n",
    "    idfModel = idf.fit(df_vect)\n",
    "    df_idf= idfModel.transform(df_vect)\n",
    "\n",
    "    normalizer = Normalizer(p=2.0, inputCol='features', outputCol='tfidf')\n",
    "    output = normalizer.transform(df_idf)\n",
    "\n",
    "    lda = LDA(k=5, maxIter=1000, featuresCol='tfidf')\n",
    "    lda_model = lda.fit(output)\n",
    "    vocab = cvmodel.vocabulary\n",
    "    #returns words for each topic term\n",
    "    def get_words(token_list):\n",
    "        return [vocab[token_id] for token_id in token_list]\n",
    "\n",
    "    udf_to_words = f.udf(get_words, t.ArrayType(t.StringType()))\n",
    "\n",
    "    #create list of topic keywords\n",
    "    # i.e topic 1 -> acer, anspire, intel\n",
    "    topics = lda_model.describeTopics(3).withColumn('topicWords', udf_to_words(f.col('termIndices'))).collect()\n",
    "    list_of_topics = []\n",
    "    for r in topics:\n",
    "        topicW = r.__getitem__('topicWords')\n",
    "        for w in topicW:\n",
    "            list_of_topics.append(w)\n",
    "\n",
    "    #returns list of 3 'hashtags' i.e keywords for topic\n",
    "    #from tokens: title, brand, cpu_brand\n",
    "    def get_key(words):\n",
    "        l = [w for w in words if w in list_of_topics]\n",
    "        l = list(set(l))\n",
    "        l.sort()\n",
    "        return l[:3]\n",
    "    udf_get_key = f.udf(get_key, t.ArrayType(t.StringType()))\n",
    "    output = output.withColumn(\"blocking_key\", udf_get_key(f.col(\"tokens_swRemoved\")))\n",
    "    output.select(\"blocking_key\").show()\n",
    "    return output\n",
    "\n",
    "\"\"\"Use universal sentence encoder from tensorflow_hub\"\"\"\n",
    "MODEL = None\n",
    "def get_model_magic():\n",
    "  global MODEL\n",
    "  if MODEL is None:\n",
    "      MODEL = hub.load(\"https://tfhub.dev/google/universal-sentence-encoder/4\")\n",
    "  return MODEL\n",
    "\n",
    "@f.udf(returnType=VectorUDT())\n",
    "def encode_sentence(x):\n",
    "  model = get_model_magic()\n",
    "  emb = model([x]).numpy()[0]\n",
    "  return Vectors.dense(emb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "columns = ['title', 'brand', 'cpu_brand', 'cpu_model', 'ram_type', 'ram_capacity', 'hdd_capacity', 'weight']\n",
    "blocking_df = tokenize(df, columns[:2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "columns = ['title', 'brand', 'cpu_brand', 'cpu_model', 'ram_type', 'ram_capacity', 'hdd_capacity', 'weight']\n",
    "for c in columns:\n",
    "    blocking_df = blocking_df.withColumn(c+'_encoding', encode_sentence(f.coalesce(f.col(c), f.lit(''))))\n",
    "\n",
    "blocking_df = generate_blocking_keys(blocking_df,\n",
    "                                     [c+'_swRemoved' for c in columns[:2]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "blocking_df.groupby('blocking_key').count().show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "# 2. Candidate pairs generation and match likelihood"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "This cell output a candidates dataframe that has\n",
    "instance_ids pairs that makes sense to compare, i.e each\n",
    "entity will be paired with another entity from the same block\n",
    "\"\"\"\n",
    "#cols_to_keep = [c+'_encoding' for c in columns]\n",
    "#for c in columns:\n",
    "#    cols_to_keep.append(c)\n",
    "#cols_to_keep.append('tokens_swRemoved')\n",
    "#cols_to_keep.append('tfidf')\n",
    "#cols_to_keep.append('instance_id')\n",
    "cols_to_keep = ['instance_id', 'title_encoding']\n",
    "#node = blocking_df.select(f.col('instance_id').alias('id'), *cols_to_keep).drop('instance_id')\n",
    "node = blocking_df.select(f.col('instance_id').alias('id'), 'title_encoding').drop('instance_id')\n",
    "node.select('title_encoding').limit(1).show()\n",
    "print(blocking_df.columns)\n",
    "pairs = blocking_df.select(*cols_to_keep, 'blocking_key')\\\n",
    "    .groupby('blocking_key').agg(f.count('instance_id').alias('size'), f.collect_set('instance_id').alias('id'))\\\n",
    "    .filter(f.col('size') > 1).select('blocking_key',f.explode('id').alias('id'))\n",
    "pairs.show()\n",
    "\n",
    "left = pairs.withColumnRenamed('id', 'src')\n",
    "right = pairs.withColumnRenamed('id', 'dst')\n",
    "#candidates based on matching of blocking_key (i.e inside the block)\n",
    "candidates = left.join(right, ['blocking_key'], 'inner')\\\n",
    "    .filter(f.col('src') < f.col('dst'))\\\n",
    "    .select('src', 'dst').distinct()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "@f.udf(returnType=t.DoubleType())\n",
    "def dot(x, y):\n",
    "  if x is not None and y is not None:\n",
    "    return float(x.dot(y))\n",
    "  else:\n",
    "    return 0\n",
    "\n",
    "def null_safe_levenshtein_sim(c1, c2):\n",
    "  output = f.when(f.col(c1).isNull() | f.col(c2).isNull(), 0)\\\n",
    "            .otherwise(1 - f.levenshtein(c1, c2) / f.greatest(f.length(c1), f.length(c2)))\n",
    "  return output\n",
    "\n",
    "def null_safe_num_sim(c1, c2):\n",
    "  output = f.when(f.col(c1).isNull() | f.col(c2).isNull(), 0)\\\n",
    "            .when((f.col(c1) == 0) & (f.col(c2) == 0), 1)\\\n",
    "            .when((f.col(c1) == 0) | (f.col(c2) == 0), 0)\\\n",
    "            .otherwise(1 - f.abs(f.col(c1) - f.col(c2)) / f.greatest(c1, c2))\n",
    "  return output\n",
    "\n",
    "def null_safe_token_overlap(c1, c2):\n",
    "  # is the overlap a significant part of the shorter string\n",
    "  output = f.when(f.col(c1).isNull() | f.col(c2).isNull(), 0)\\\n",
    "            .when((f.size(f.array_distinct(c1)) == 0) | (f.size(f.array_distinct(c2)) == 0), 0)\\\n",
    "            .otherwise(f.size(f.array_intersect(c1, c2)) / f.least(f.size(f.array_distinct(c1)), f.size(f.array_distinct(c1))))\n",
    "  return output\n",
    "\n",
    "def calc_sim(df, candidates):\n",
    "    metrics = []\n",
    "    for c in columns[:2]:\n",
    "        if '_encoding' not in c:\n",
    "            candidates = candidates.withColumn(c+'_lev', null_safe_levenshtein_sim(df.filter(df.id == candidates.src).select(c),df.filter(df.id == candidates.dst).select(c)))\n",
    "            metrics.append(c+'_lev')\n",
    "        else:\n",
    "            metrics.append(c+'_sim')\n",
    "            candidates = candidates.withColumn(c+'_sim', dot(df.filter(df.id == candidates.src).select(c), df.filter(df.id == candidates.dst).select(c)))\n",
    "    candidates = candidates.withColumn('tfidf_sim', dot(df.filter(df.id == candidates.src).select('tfidf'),df.filter(df.id == candidates.dst).select('tfidf')))\n",
    "    candidates = candidates.withColumn('token_sim', dot(df.filter(df.id == candidates.src).select('tokens_swRemoved'), df.filter(df.id == candidates.dst).select('tokens_swRemoved')))\n",
    "    candidates = candidates.withColumn('weight_sim', dot(df.filter(df.id == candidates.src).select('weight'),df.filter(df.id == candidates.dst).select('weight')))\n",
    "    metrics.append('tfidf_sim')\n",
    "    metrics.append('token_sim')\n",
    "    metrics.append('weigth_sim')\n",
    "    def sum_distance(distances):\n",
    "        return sum(d for d in distances)\n",
    "    udf_sum = f.udf(sum_distance, t.DoubleType())\n",
    "    candidates = candidates.withColumn('sum_sim', udf_sum([f.col(c) for c in metrics]))\n",
    "    udf_norm = f.udf(lambda d : d / len(metrics))\n",
    "    candidates = candidates.withColumn('overall_sim', udf_norm(f.col('sum_sim'))).drop(f.col('sum_sim'))\n",
    "    return candidates\n",
    "\n",
    "distance_df = calc_sim(node, candidates)\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "labels = spark.read.csv(\"data/Y2.csv\", header=True).withColumnRenamed('left_instance_id', 'lid').withColumnRenamed('right_instance_id', 'rid')\n",
    "labels.show()\n",
    "#label_df = labels.join(candidates.withColumnRenamed('src','lid').withColumnRenamed('dst','rid'), ['lid','rid'], 'inner')\n",
    "label_df = labels.join(node.alias(\"node_1\"), labels.lid == node.id, 'inner').drop('id')\n",
    "for c in cols_to_keep[1:]:\n",
    "    label_df = label_df.withColumnRenamed(c, 'l_'+c)\n",
    "print(label_df.columns)\n",
    "label_df = label_df.alias('one').join(node.alias(\"node_2\"), label_df.rid == node.id, 'inner').drop('id')\n",
    "for c in cols_to_keep[1:]:\n",
    "    label_df = label_df.withColumnRenamed(c, 'r_'+c)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def toList(row):\n",
    "    l = []\n",
    "    for v in row:\n",
    "        for n in v:\n",
    "            l.append(float(n))\n",
    "    return l\n",
    "udf_toList = f.udf(toList, t.ArrayType(t.FloatType()))\n",
    "label_df = label_df.withColumn('features', udf_toList(f.array('l_title_encoding', 'r_title_encoding')))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}