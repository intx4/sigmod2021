{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "name": "pycharm-a2066b67",
      "language": "python",
      "display_name": "PyCharm (sigmod-2021)"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.5"
    },
    "colab": {
      "name": "main.ipynb",
      "provenance": []
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "g6Wjqcnc6oEe"
      },
      "source": [
        "!pip install pyspark graphframes"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yB4SXU_m7w-Q"
      },
      "source": [
        "!export PYSPARK_SUBMIT_ARGS='--packages graphframes:graphframes:0.8.1-spark3.0-s_2.12'"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "pycharm": {
          "name": "#%%\n"
        },
        "id": "brY-5_-X6g6p"
      },
      "source": [
        "from pyspark.conf import SparkConf\n",
        "from pyspark.sql import SparkSession\n",
        "import tensorflow_hub as hub\n",
        "from pyspark.sql import functions as f\n",
        "from pyspark.sql import types as t\n",
        "from pyspark.sql import Window as w\n",
        "from graphframes import GraphFrame\n",
        "from pyspark.ml.linalg import DenseVector, SparseVector\n",
        "from pyspark.ml.feature import HashingTF, IDF, Tokenizer, RegexTokenizer, CountVectorizer, StopWordsRemover, NGram, Normalizer, VectorAssembler, Word2Vec, Word2VecModel, PCA\n",
        "from pyspark.ml.clustering import LDA\n",
        "from pyspark.ml.linalg import VectorUDT, Vectors"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "pycharm": {
          "name": "#%%\n"
        },
        "id": "UqZuRaaW6g6r"
      },
      "source": [
        "spark = (\n",
        "    SparkSession.builder\n",
        "    .config('spark.executor.memory', '4g')\n",
        "    .config('spark.app.name', 'Spark Updated Conf')\n",
        "    .config('spark.executor.cores', '2')\n",
        "    .config('spark.cores.max', '2')\n",
        "    .config('spark.driver.memory','8g')\n",
        "    .getOrCreate()\n",
        ")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "pycharm": {
          "name": "#%% md\n"
        },
        "id": "uB4VNsak6g6t"
      },
      "source": [
        "# 0. Load data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "pycharm": {
          "name": "#%%\n"
        },
        "id": "PaEElmjg6g6u"
      },
      "source": [
        "df = spark.read.csv(\"X2.csv\", header=True)\n",
        "df.toPandas()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "pycharm": {
          "name": "#%% md\n"
        },
        "id": "nj-gVsHx6g6v"
      },
      "source": [
        "# 0. Data cleaning"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "pycharm": {
          "name": "#%%\n"
        },
        "id": "Pasvnmhd6g6w"
      },
      "source": [
        "# Set everything to lowercase\n",
        "for c in df.columns:\n",
        "    df = df.withColumn(c, f.lower(f.col(c)))\n",
        "\n",
        "# Extract brand or infer from title\n",
        "df = df.drop('ssd_capacity')\n",
        "df = df.withColumn('brand', f.regexp_extract('brand', \"^(\\w+)\", 0))\n",
        "computer_brands = ['lenovo', 'acer', 'hp', 'dell', 'asus', 'samsung', 'huawei', 'surface', 'apple']\n",
        "computer_brands_pattern = '({})'.format('|'.join(computer_brands))\n",
        "df = df.withColumn('brand', f.when( f.regexp_extract('title', computer_brands_pattern, 0)!='', f.regexp_extract('title', computer_brands_pattern, 0))\\\n",
        "                   .otherwise(df.brand))\n",
        "#exctract cpu_brand and infer type if intel\n",
        "cpu_brands = ['intel', 'apple', 'amd', 'nvidia', 'arm']\n",
        "cpu_pattern = '({})'.format('|'.join(cpu_brands))\n",
        "df = df.withColumn('cpu_model',f.regexp_extract('cpu_model', '(i\\d|pentium|celeron|a\\d)', 0))\n",
        "df = df.withColumn('cpu_model', f.when( (f.regexp_extract('cpu_brand','(intel|amd)', 0 )!='') & f.isnull(df.cpu_model) ,\\\n",
        "                                        f.regexp_extract('cpu_brand', '(i\\d|pentium|celeron|a\\d)', 0))\\\n",
        "                   .otherwise(df.cpu_model))\n",
        "df = df.withColumn('cpu_brand', f.when(f.regexp_extract('cpu_brand', cpu_pattern, 0) != '', f.regexp_extract('cpu_brand', cpu_pattern, 1))\\\n",
        "                                       .otherwise(f.regexp_extract('title', cpu_pattern, 0)))\n",
        "df = df.withColumn('weight', f.when(df.weight.contains('pounds') | df.weight.contains('lbs'),\n",
        "                                    (f.regexp_extract('weight', '(\\d+.?\\d)', 0).cast(t.DoubleType()))).otherwise(\n",
        "                                    f.round(f.regexp_extract('weight', '(\\d+.?\\d)', 0).cast(t.DoubleType())*2.20462,1)\n",
        "                        )\n",
        "                    )\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tNIZDK-K-EGd"
      },
      "source": [
        "from more_itertools import intersperse\n",
        "\n",
        "def merge_columns(df, column_names, output):\n",
        "    df = df.withColumn(output, f.concat_ws(\" \", *column_names))\n",
        "    return df.drop(*column_names)\n",
        "\n",
        "ddf = df.drop(\"ram_frequency\")\n",
        "ddf = merge_columns(ddf, [\"cpu_brand\", \"cpu_model\", \"cpu_frequency\", \"cpu_type\"], \"cpu\")\n",
        "ddf = merge_columns(ddf, [\"ram_capacity\", \"ram_type\"], \"ram\")\n",
        "ddf.toPandas()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "y5lLUdrr6g6w"
      },
      "source": [
        "# 1. Blocking\n",
        "Blocking will be done feeding a TF-IDF matrix to an LDA model and extracting\n",
        "keywords from the title matching them to topics."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "pycharm": {
          "name": "#%%\n"
        },
        "id": "qEBkED7C6g6x"
      },
      "source": [
        "@f.udf(returnType=t.ArrayType(t.StringType()))\n",
        "def filter_alnum(arr):\n",
        "    return [t for t in arr if t.isalnum() and len(t) > 2]\n",
        "\n",
        "\"\"\"Returns the df with tokenized columns with stopwords removed\"\"\"\n",
        "def tokenize(df, string_cols):\n",
        "    output = df\n",
        "    stopW = ['softwarecity', 'amazon', 'com', 'pc', 'windows', 'computers', 'computer', 'accessories', 'laptop', 'notebook', 'kg', 'inch', 'processor', 'memory','gb', 'ram', 'hdd', 'ssd', 'cpu', 'display', 'hz', 'ghz', 'tb','rpm', 'slot', 'slots', 'mhz', 'cache', 'ram', 'ddram', 'dram', 'hd']\n",
        "    for c in string_cols:\n",
        "        output = output.withColumn('temp', f.coalesce(f.col(c), f.lower(c), f.lit('')))\n",
        "        tokenizer = RegexTokenizer(inputCol='temp', outputCol=c+\"_rawtokens\", pattern = \"\\\\W\")\n",
        "        remover = StopWordsRemover(inputCol=c+\"_rawtokens\", outputCol=c+\"_tokens\", stopWords=stopW)\n",
        "\n",
        "        output = tokenizer.transform(output)\n",
        "        output = remover.transform(output).drop(c+\"_rawtokens\")\n",
        "        output = output.withColumn(c+'_tokens', f.array_distinct(filter_alnum(f.col(c+\"_tokens\"))))\n",
        "    # output has c+tokens columns\n",
        "    return output.drop(\"temp\")\n",
        "\n",
        "def generate_blocking_keys(df, token_cols, min_freq=1):\n",
        "    \"\"\"Pipeline:\n",
        "            1 - CountVectorizer -> TF\n",
        "            2 - IDF\n",
        "            3 - LDA\n",
        "    \"\"\"\n",
        "    df = df.withColumn('tokens', f.array_distinct(f.concat(*token_cols)))\n",
        "    cv = CountVectorizer(inputCol='tokens', outputCol=\"raw_features\")\n",
        "    cvmodel = cv.fit(df)\n",
        "    df_vect = cvmodel.transform(df)\n",
        "\n",
        "    idf = IDF(inputCol=\"raw_features\", outputCol=\"features\", minDocFreq=min_freq)\n",
        "    idfModel = idf.fit(df_vect)\n",
        "    df_idf= idfModel.transform(df_vect)\n",
        "\n",
        "    normalizer = Normalizer(p=2.0, inputCol='features', outputCol='tfidf')\n",
        "    output = normalizer.transform(df_idf)\n",
        "\n",
        "    lda = LDA(k=5, maxIter=1000, featuresCol='tfidf')\n",
        "    lda_model = lda.fit(output)\n",
        "    vocab = cvmodel.vocabulary\n",
        "    #returns words for each topic term\n",
        "    @f.udf(returnType=t.ArrayType(t.StringType()))\n",
        "    def get_words(token_list):\n",
        "        return [vocab[token_id] for token_id in token_list]\n",
        "\n",
        "    #create list of topic keywords\n",
        "    # i.e topic 1 -> acer, anspire, intel\n",
        "    topics = lda_model.describeTopics(3).withColumn('topicWords', get_words(f.col('termIndices'))).collect()\n",
        "    list_of_topics = []\n",
        "    for r in topics:\n",
        "        topicW = r.__getitem__('topicWords')\n",
        "        for w in topicW:\n",
        "            list_of_topics.append(w)\n",
        "\n",
        "    #returns list of 3 'hashtags' i.e keywords for topic\n",
        "    #from tokens: title, brand, cpu_brand\n",
        "    @f.udf(returnType=t.ArrayType(t.StringType()))\n",
        "    def get_key(words):\n",
        "        l = [w for w in words if w in list_of_topics]\n",
        "        l = list(set(l))\n",
        "        l.sort()\n",
        "        return l[:3]\n",
        "    output = output.withColumn(\"blocking_key\", get_key(f.col(\"tokens\")))\n",
        "    return output\n",
        "\n",
        "\"\"\"Use universal sentence encoder from tensorflow_hub\"\"\"\n",
        "MODEL = None\n",
        "def get_model_magic():\n",
        "  global MODEL\n",
        "  if MODEL is None:\n",
        "      MODEL = hub.load(\"https://tfhub.dev/google/universal-sentence-encoder/4\")\n",
        "  return MODEL\n",
        "\n",
        "@f.udf(returnType=VectorUDT())\n",
        "def encode_sentence(x):\n",
        "  model = get_model_magic()\n",
        "  emb = model([x]).numpy()[0]\n",
        "  return Vectors.dense(emb)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "pycharm": {
          "name": "#%%\n"
        },
        "id": "ouaaVLVQ6g6y"
      },
      "source": [
        "columns = ['title', 'cpu', 'ram']\n",
        "#Generate Blocking Keys\n",
        "\n",
        "blocking_df = tokenize(ddf, columns)\n",
        "#for c in columns:\n",
        "    #blocking_df = blocking_df.withColumn(c+'_encoding', encode_sentence(f.coalesce(f.col(c), f.lit(''))))\n",
        "blocking_df = generate_blocking_keys(blocking_df, [c+'_tokens' for c in columns])\n",
        "\n",
        "#Add encoding on title\n",
        "blocking_df = blocking_df.withColumn('title_encoding', encode_sentence(f.coalesce(f.col('title'), f.lit(''))))\n",
        "\n",
        "blocking_df.toPandas()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "pycharm": {
          "name": "#%%\n"
        },
        "id": "AfgQ2rjy6g6z"
      },
      "source": [
        "blocking_df.groupby('blocking_key').count().show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "pycharm": {
          "name": "#%% md\n"
        },
        "id": "ewM2XlPt6g60"
      },
      "source": [
        "# 2. Candidate pairs generation and match likelihood"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "pycharm": {
          "name": "#%%\n"
        },
        "id": "g_FB3bXg6g60"
      },
      "source": [
        "\"\"\"\n",
        "This cell output a candidates dataframe that has\n",
        "instance_ids pairs that makes sense to compare, i.e each\n",
        "entity will be paired with another entity from the same block\n",
        "\"\"\"\n",
        "\n",
        "cols_to_keep = [\"instance_id\", \"title_encoding\"]\n",
        "# Filter blocks to only keep ones bigger than one\n",
        "pairs = (\n",
        "    blocking_df\n",
        "    .select(*cols_to_keep, 'blocking_key')\n",
        "    .groupby('blocking_key').agg(f.count('instance_id').alias('size'), f.collect_set('instance_id').alias('id'))\\\n",
        "    .filter(f.col('size') > 1).select('blocking_key',f.explode('id').alias('id'))\n",
        ")\n",
        "\n",
        "left = pairs.withColumnRenamed('id', 'src')\n",
        "right = pairs.withColumnRenamed('id', 'dst')\n",
        "#candidates based on matching of blocking_key (i.e inside the block)\n",
        "candidates = left.join(right, ['blocking_key'], 'inner')\\\n",
        "    .filter(f.col('src') < f.col('dst'))\\\n",
        "    .select('src', 'dst').distinct()\n",
        "node = blocking_df.select(f.col('instance_id').alias('id'), 'title_encoding').drop('instance_id')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "pycharm": {
          "name": "#%%\n"
        },
        "id": "1rMiiAQC6g60"
      },
      "source": [
        "\"\"\"\n",
        "@f.udf(returnType=t.DoubleType())\n",
        "def dot(x, y):\n",
        "  if x is not None and y is not None:\n",
        "    return float(x.dot(y))\n",
        "  else:\n",
        "    return 0\n",
        "\n",
        "def null_safe_levenshtein_sim(c1, c2):\n",
        "  output = f.when(f.col(c1).isNull() | f.col(c2).isNull(), 0)\\\n",
        "            .otherwise(1 - f.levenshtein(c1, c2) / f.greatest(f.length(c1), f.length(c2)))\n",
        "  return output\n",
        "\n",
        "def null_safe_num_sim(c1, c2):\n",
        "  output = f.when(f.col(c1).isNull() | f.col(c2).isNull(), 0)\\\n",
        "            .when((f.col(c1) == 0) & (f.col(c2) == 0), 1)\\\n",
        "            .when((f.col(c1) == 0) | (f.col(c2) == 0), 0)\\\n",
        "            .otherwise(1 - f.abs(f.col(c1) - f.col(c2)) / f.greatest(c1, c2))\n",
        "  return output\n",
        "\n",
        "def null_safe_token_overlap(c1, c2):\n",
        "  # is the overlap a significant part of the shorter string\n",
        "  output = f.when(f.col(c1).isNull() | f.col(c2).isNull(), 0)\\\n",
        "            .when((f.size(f.array_distinct(c1)) == 0) | (f.size(f.array_distinct(c2)) == 0), 0)\\\n",
        "            .otherwise(f.size(f.array_intersect(c1, c2)) / f.least(f.size(f.array_distinct(c1)), f.size(f.array_distinct(c1))))\n",
        "  return output\n",
        "\n",
        "def calc_sim(df, candidates):\n",
        "    metrics = []\n",
        "    for c in columns[:2]:\n",
        "        if '_encoding' not in c:\n",
        "            candidates = candidates.withColumn(c+'_lev', null_safe_levenshtein_sim(df.filter(df.id == candidates.src).select(c),df.filter(df.id == candidates.dst).select(c)))\n",
        "            metrics.append(c+'_lev')\n",
        "        else:\n",
        "            metrics.append(c+'_sim')\n",
        "            candidates = candidates.withColumn(c+'_sim', dot(df.filter(df.id == candidates.src).select(c), df.filter(df.id == candidates.dst).select(c)))\n",
        "    candidates = candidates.withColumn('tfidf_sim', dot(df.filter(df.id == candidates.src).select('tfidf'),df.filter(df.id == candidates.dst).select('tfidf')))\n",
        "    candidates = candidates.withColumn('token_sim', dot(df.filter(df.id == candidates.src).select('tokens_swRemoved'), df.filter(df.id == candidates.dst).select('tokens_swRemoved')))\n",
        "    candidates = candidates.withColumn('weight_sim', dot(df.filter(df.id == candidates.src).select('weight'),df.filter(df.id == candidates.dst).select('weight')))\n",
        "    metrics.append('tfidf_sim')\n",
        "    metrics.append('token_sim')\n",
        "    metrics.append('weigth_sim')\n",
        "    def sum_distance(distances):\n",
        "        return sum(d for d in distances)\n",
        "    udf_sum = f.udf(sum_distance, t.DoubleType())\n",
        "    candidates = candidates.withColumn('sum_sim', udf_sum([f.col(c) for c in metrics]))\n",
        "    udf_norm = f.udf(lambda d : d / len(metrics))\n",
        "    candidates = candidates.withColumn('overall_sim', udf_norm(f.col('sum_sim'))).drop(f.col('sum_sim'))\n",
        "    return candidates\n",
        "\n",
        "distance_df = calc_sim(node, candidates)\n",
        "\"\"\""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "pycharm": {
          "name": "#%%\n"
        },
        "id": "dLFCsO_D6g61"
      },
      "source": [
        "\"\"\"\n",
        "Read label.csv and expand it trough transitivity\n",
        "\"\"\"\n",
        "labels = (\n",
        "    spark.read.csv(\"Y2.csv\", header=True)\n",
        "    .withColumnRenamed('left_instance_id', 'lid')\n",
        "    .withColumnRenamed('right_instance_id', 'rid')\n",
        ")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "pycharm": {
          "name": "#%%\n"
        },
        "id": "LV2XM0oB6g62"
      },
      "source": [
        "\"\"\"\n",
        "Reuse this cell to join a <left_id,right_id> with node to extract features\n",
        "\"\"\"\n",
        "#label_df = labels.join(candidates.withColumnRenamed('src','lid').withColumnRenamed('dst','rid'), ['lid','rid'], 'inner')\n",
        "label_df = labels.join(node.alias(\"node_1\"), labels.lid == node.id, 'inner').drop('id')\n",
        "for c in cols_to_keep[1:]:\n",
        "    label_df = label_df.withColumnRenamed(c, 'l_'+c)\n",
        "\n",
        "label_df = label_df.alias('one').join(node.alias(\"node_2\"), label_df.rid == node.id, 'inner').drop('id')\n",
        "for c in cols_to_keep[1:]:\n",
        "    label_df = label_df.withColumnRenamed(c, 'r_'+c)\n",
        "print(label_df.columns)\n",
        "\n",
        "matching_pairs = candidates.join(node.alias(\"node_1\"), candidates.src == node.id, 'inner').drop('id')\n",
        "for c in cols_to_keep[1:]:\n",
        "    matching_pairs = matching_pairs.withColumnRenamed(c, 'l_'+c)\n",
        "\n",
        "matching_pairs = matching_pairs.alias('one').join(node.alias(\"node_2\"), matching_pairs.dst == node.id, 'inner').drop('id')\n",
        "for c in cols_to_keep[1:]:\n",
        "    matching_pairs = matching_pairs.withColumnRenamed(c, 'r_'+c)\n",
        "\n",
        "matching_pairs.columns"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "pycharm": {
          "name": "#%%\n"
        },
        "id": "trF-IsGj6g62"
      },
      "source": [
        "@f.udf(returnType=VectorUDT())\n",
        "def toList(row):\n",
        "    l = []\n",
        "    for v in row:\n",
        "        for n in v:\n",
        "            l.append(float(n))\n",
        "    return Vectors.dense(l)\n",
        "\n",
        "label_df = label_df.withColumn('features', toList(f.array('l_title_encoding', 'r_title_encoding')))\\\n",
        "    .drop('l_title_encoding', 'r_title_encoding')\n",
        "label_df = label_df.withColumn('label', f.col('label').cast(t.IntegerType()))\n",
        "label_df = label_df.withColumn('weights', f.when(f.col('label')==0, 0.66).otherwise(1.0))\n",
        "\n",
        "matching_pairs = matching_pairs.withColumn('features', toList(f.array('l_title_encoding', 'r_title_encoding')))\\\n",
        "    .drop('l_title_encoding', 'r_title_encoding')\n",
        "\n",
        "matching_pairs = matching_pairs.withColumnRenamed('src', 'lid').withColumnRenamed('dst','rid')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mla9wzTj6g62"
      },
      "source": [
        "# 3. Machine Learning Magic Bitch"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "pycharm": {
          "name": "#%%\n"
        },
        "id": "YP1eg3sE6g62"
      },
      "source": [
        "from pyspark.ml.classification import LinearSVC, LogisticRegression\n",
        "from pyspark.ml.tuning import ParamGridBuilder, CrossValidator\n",
        "from pyspark.ml.evaluation import BinaryClassificationEvaluator"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "pycharm": {
          "name": "#%%\n"
        },
        "id": "scCBn8BJ6g63"
      },
      "source": [
        "model = LinearSVC(featuresCol='features', labelCol='label', weightCol='weights',maxIter=100)\n",
        "param_grid = ParamGridBuilder().addGrid(model.regParam, [0.5, 0.4, 0.3, 0.2, 0.1]).build()\n",
        "cvs = CrossValidator(estimator=model,\n",
        "                           estimatorParamMaps=param_grid,\n",
        "                           evaluator=BinaryClassificationEvaluator(),#(rawPredictionCol='prediction', labelCol='label'),\\\n",
        "                           numFolds=4)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "pycharm": {
          "name": "#%%\n"
        },
        "id": "EjGJSEK46g63"
      },
      "source": [
        "training_set, test_set = label_df.randomSplit([0.8, 0.2])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "pycharm": {
          "name": "#%%\n"
        },
        "id": "3E_ceDGU6g63"
      },
      "source": [
        "#grid_search, hyperpar tuning...\n",
        "estimator = cvs.fit(training_set)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "pycharm": {
          "name": "#%%\n"
        },
        "id": "G_EWxg696g63"
      },
      "source": [
        "prediction = estimator.transform(test_set).select('lid','rid','label','prediction')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "coxTEkuVeP8o"
      },
      "source": [
        "prediction.groupby(\"label\").count().toPandas()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "faNzcXmMfD9t"
      },
      "source": [
        "estimator.save(\"model.model\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fWwCDyjZfN4L"
      },
      "source": [
        "!zip -r model.zip model.model"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "pycharm": {
          "name": "#%%\n"
        },
        "id": "MCF1dofN6g64"
      },
      "source": [
        "accuracy = prediction.filter(f.col('label')==f.col('prediction').cast(t.IntegerType())).count() / prediction.count()\n",
        "print(\"Accuracy: \", accuracy)"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}