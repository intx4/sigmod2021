{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 0. Load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "import tensorflow_hub as hub\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql import functions as f\n",
    "from pyspark.sql import types as t\n",
    "from pyspark.sql import Window as w\n",
    "\n",
    "from pyspark.ml.linalg import DenseVector, SparseVector\n",
    "from pyspark.ml.feature import HashingTF, IDF,  Tokenizer, RegexTokenizer, CountVectorizer, StopWordsRemover, NGram, Normalizer, VectorAssembler, Word2Vec, Word2VecModel, PCA\n",
    "from pyspark.ml import Pipeline, Transformer\n",
    "from pyspark.ml.clustering import LDA\n",
    "from pyspark.mllib.linalg import Vectors, VectorUDT\n",
    "\n",
    "spark = SparkSession.builder.getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "df = spark.read.csv(\"data/X2.csv\", header=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "# 0. Data cleaning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "for c in df.columns:\n",
    "#set everything to lowercase\n",
    "    df = df.withColumn(c, f.lower(f.col(c)))\n",
    "\n",
    "#extract brand or infer from title\n",
    "df = df.withColumn('brand', f.regexp_extract('brand', \"^(\\w+)\", 0))\n",
    "computer_brands = ['(lenovo', 'acer', 'hp', 'dell', 'asus', 'samsung', 'huawei', 'surface', 'apple)']\n",
    "computer_brands_pattern = '|'.join(computer_brands)\n",
    "df = df.withColumn('brand', f.when( f.regexp_extract('title', computer_brands_pattern, 0)!='', f.regexp_extract('title', computer_brands_pattern, 0))\\\n",
    "                   .otherwise(df.brand))\n",
    "#exctract cpu_brand and infer type if intel\n",
    "cpu_brands = ['(intel', 'apple', 'amd', 'nvidia', 'arm)']\n",
    "cpu_pattern = '|'.join(cpu_brands)\n",
    "df = df.withColumn('cpu_model', f.when( (f.regexp_extract('cpu_brand','intel', 0 )!='') & f.isnull(df.cpu_model) ,\\\n",
    "                                        f.regexp_extract('cpu_brand', '(i\\d|pentium|celeron)', 0))\\\n",
    "                   .otherwise(df.cpu_model))\n",
    "df = df.withColumn('cpu_brand', f.when(f.regexp_extract('cpu_brand', cpu_pattern, 0) != '', f.regexp_extract('cpu_brand', cpu_pattern, 1))\\\n",
    "                                       .otherwise(f.regexp_extract('title', cpu_pattern, 0)))\n",
    "df = df.withColumn('weight', f.when(df.weight.contains('pounds') | df.weight.contains('lbs'),\n",
    "                                    (f.regexp_extract('weight', '(\\d+\\.?\\d*)', 0).cast(t.DoubleType()))).otherwise(\n",
    "                                    f.round(f.regexp_extract('weight', '(\\d+\\.?\\d*)', 0).cast(t.DoubleType())*2.20462,1)\n",
    "                        )\n",
    "                    )\n",
    "df.select('weight').show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Blocking"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "\"\"\"UTILITIES\"\"\"\n",
    "\n",
    "\"\"\"Returns the df with tokenized columns with stopwords removed\"\"\"\n",
    "def tokenize(df, string_cols):\n",
    "  output = df\n",
    "  stopW = ['softwarecity', 'amazon', 'com','pc', 'windows', 'computers', 'computer', 'accessories', 'laptop', 'notebook', 'kg', 'inch', 'processor', 'memory','gb', 'ram', 'hdd', 'ssd', 'cpu', 'display', 'hz', 'ghz', 'tb', 'mhz', 'cache', 'ram', 'ddram', 'dram', 'hd']\n",
    "  for c in string_cols:\n",
    "    output = output.withColumn('temp', f.coalesce(f.col(c), f.lower(c), f.lit('')))\n",
    "    tokenizer = RegexTokenizer(inputCol='temp', outputCol=c+\"_tokens\", pattern = \"\\\\W\")\n",
    "    remover = StopWordsRemover(inputCol=c+\"_tokens\", outputCol=c+\"_swRemoved\", stopWords=stopW)\n",
    "    output = tokenizer.transform(output)\n",
    "\n",
    "    filter_alnum = f.udf(lambda l : [t for t in l[:10] if t.isalpha() and len(t) >= 2], t.ArrayType(t.StringType()))\n",
    "    output = output.withColumn(c+'_tokens', filter_alnum(f.col(c+\"_tokens\")))\n",
    "\n",
    "    output.select(c+'_tokens').show()\n",
    "\n",
    "    output = remover.transform(output)\\\n",
    "      .drop('temp', c+\"_tokens\")\n",
    "    # output has c+swRemoved columns\n",
    "    output.select(c+'_swRemoved').show()\n",
    "\n",
    "  return output\n",
    "\n",
    "def generate_blocking_keys(df, token_cols, min_freq=1):\n",
    "    \"\"\"Pipeline:\n",
    "            1 - CountVectorizer -> TF\n",
    "            2 - IDF\n",
    "            3 - LDA\n",
    "    \"\"\"\n",
    "    def assemble_tokens(list_of_lists):\n",
    "        tokens = []\n",
    "        for l in list_of_lists:\n",
    "            for t in l:\n",
    "                tokens.append(t)\n",
    "        return tokens\n",
    "    udf_assemble = f.udf(assemble_tokens, t.ArrayType(t.StringType()))\n",
    "    df = df.withColumn('tokens_swRemoved', f.concat(*token_cols))\n",
    "    cv = CountVectorizer(inputCol='tokens_swRemoved', outputCol=\"rawFeatures\")\n",
    "    cvmodel = cv.fit(df)\n",
    "    df_vect = cvmodel.transform(df)\n",
    "\n",
    "    idf = IDF(inputCol=\"rawFeatures\", outputCol=\"features\", minDocFreq=min_freq)\n",
    "    idfModel = idf.fit(df_vect)\n",
    "    df_idf= idfModel.transform(df_vect)\n",
    "\n",
    "    normalizer = Normalizer(p=2.0, inputCol='features', outputCol='tfidf')\n",
    "    output = normalizer.transform(df_idf)\n",
    "\n",
    "    lda = LDA(k=3, maxIter=500, featuresCol='tfidf')\n",
    "    lda_model = lda.fit(output)\n",
    "    vocab = cvmodel.vocabulary\n",
    "\n",
    "    def get_words(token_list):\n",
    "        return [vocab[token_id] for token_id in token_list]\n",
    "\n",
    "    udf_to_words = f.udf(get_words, t.ArrayType(t.StringType()))\n",
    "\n",
    "    topics = lda_model.describeTopics(2).withColumn('topicWords', udf_to_words(f.col('termIndices')))\\\n",
    "        .show()\n",
    "    output = lda_model.transform(output)\n",
    "    output.select('topicDistribution').show()\n",
    "\n",
    "    #returns the index of the topic for which the distr is higher\n",
    "    def get_key(topicDistr):\n",
    "        max_i = 0\n",
    "        max = 0.0\n",
    "        i = 0\n",
    "        for t in topicDistr:\n",
    "            if t >= max:\n",
    "                max = t\n",
    "                max_i = i\n",
    "            i += 1\n",
    "        return max_i\n",
    "\n",
    "    udf_get_key = f.udf(get_key, t.IntegerType())\n",
    "    output = output.withColumn(\"blocking_key\", udf_get_key(f.col(\"topicDistribution\")))\n",
    "    output.select(\"blocking_key\").show()\n",
    "    return output\n",
    "\n",
    "\"\"\"Use universal sentence encoder from tensorflow_hub\"\"\"\n",
    "MODEL = None\n",
    "def get_model_magic():\n",
    "  global MODEL\n",
    "  if MODEL is None:\n",
    "      MODEL = hub.load(\"https://tfhub.dev/google/universal-sentence-encoder/4\")\n",
    "  return MODEL\n",
    "\n",
    "@f.udf(returnType=VectorUDT())\n",
    "def encode_sentence(x):\n",
    "  model = get_model_magic()\n",
    "  emb = model([x]).numpy()[0]\n",
    "  return Vectors.dense(emb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "columns = ['brand','cpu_brand','title']\n",
    "blocking_df = tokenize(df, columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "blocking_df = generate_blocking_keys(blocking_df, [c+'_swRemoved' for c in columns])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "blocking_df.groupby('blocking_key').count().show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "# 2. Canidate pairs match generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from graphframes import GraphFrame\n",
    "\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}