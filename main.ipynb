{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Cannot run multiple SparkContexts at once; existing SparkContext(app=sigmod-21, master=local) created by __init__ at <ipython-input-1-30232563b400>:16 ",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mValueError\u001B[0m                                Traceback (most recent call last)",
      "\u001B[0;32m<ipython-input-23-04eb7c37d502>\u001B[0m in \u001B[0;36m<module>\u001B[0;34m\u001B[0m\n\u001B[1;32m     14\u001B[0m conf = (SparkConf().setMaster('local').setAppName('sigmod-21').set('spark.executor.memory', '4G').set('spark.driver.memory', '4G')\\\n\u001B[1;32m     15\u001B[0m         .set('spark.sql.broadcastTimeout', '1000'))\n\u001B[0;32m---> 16\u001B[0;31m \u001B[0msc\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mSparkContext\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mconf\u001B[0m\u001B[0;34m=\u001B[0m\u001B[0mconf\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m     17\u001B[0m \u001B[0mspark\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mSparkSession\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mbuilder\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mgetOrCreate\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m     18\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m~/.local/lib/python3.8/site-packages/pyspark/context.py\u001B[0m in \u001B[0;36m__init__\u001B[0;34m(self, master, appName, sparkHome, pyFiles, environment, batchSize, serializer, conf, gateway, jsc, profiler_cls)\u001B[0m\n\u001B[1;32m    142\u001B[0m                 \" is not allowed as it is a security risk.\")\n\u001B[1;32m    143\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m--> 144\u001B[0;31m         \u001B[0mSparkContext\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0m_ensure_initialized\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mself\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mgateway\u001B[0m\u001B[0;34m=\u001B[0m\u001B[0mgateway\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mconf\u001B[0m\u001B[0;34m=\u001B[0m\u001B[0mconf\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m    145\u001B[0m         \u001B[0;32mtry\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    146\u001B[0m             self._do_init(master, appName, sparkHome, pyFiles, environment, batchSize, serializer,\n",
      "\u001B[0;32m~/.local/lib/python3.8/site-packages/pyspark/context.py\u001B[0m in \u001B[0;36m_ensure_initialized\u001B[0;34m(cls, instance, gateway, conf)\u001B[0m\n\u001B[1;32m    340\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    341\u001B[0m                     \u001B[0;31m# Raise error if there is already a running Spark context\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m--> 342\u001B[0;31m                     raise ValueError(\n\u001B[0m\u001B[1;32m    343\u001B[0m                         \u001B[0;34m\"Cannot run multiple SparkContexts at once; \"\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    344\u001B[0m                         \u001B[0;34m\"existing SparkContext(app=%s, master=%s)\"\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;31mValueError\u001B[0m: Cannot run multiple SparkContexts at once; existing SparkContext(app=sigmod-21, master=local) created by __init__ at <ipython-input-1-30232563b400>:16 "
     ]
    }
   ],
   "source": [
    "from pyspark import SparkContext, SparkConf\n",
    "import tensorflow_hub as hub\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql import functions as f\n",
    "from pyspark.sql import types as t\n",
    "from pyspark.sql import Window as w\n",
    "from graphframes import GraphFrame\n",
    "from pyspark.ml.linalg import DenseVector, SparseVector\n",
    "from pyspark.ml.feature import HashingTF, IDF, Tokenizer, RegexTokenizer, CountVectorizer, StopWordsRemover, NGram, Normalizer, VectorAssembler, Word2Vec, Word2VecModel, PCA\n",
    "\n",
    "from pyspark.ml.clustering import LDA\n",
    "from pyspark.ml.linalg import VectorUDT, Vectors\n",
    "\n",
    "conf = (SparkConf().setMaster('local').setAppName('sigmod-21').set('spark.executor.memory', '4G').set('spark.driver.memory', '4G')\\\n",
    "        .set('spark.sql.broadcastTimeout', '1000'))\n",
    "sc = SparkContext(conf=conf)\n",
    "spark = SparkSession.builder.getOrCreate()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "# 0. Load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": "                  instance_id  \\\n0    www.softwarecity.ca//737   \n1    www.isupplyhub.com//1256   \n2     www.isupplyhub.com//326   \n3     www.isupplyhub.com//821   \n4     www.isupplyhub.com//157   \n..                        ...   \n338       www.vology.com//873   \n339       www.vology.com//823   \n340      www.vology.com//2723   \n341      www.vology.com//1349   \n342      www.vology.com//3017   \n\n                                                 brand  \\\n0                                               Lenovo   \n1                                                 Acer   \n2                                                 Acer   \n3                                                   HP   \n4                                                 Asus   \n..                                                 ...   \n338  Lenovo ThinkPad X230 2320 - 12.5 '' - Core i5 ...   \n339  Lenovo ThinkPad X230 2325 - 12.5 '' - Core i5 ...   \n340  Lenovo ThinkPad X230 Tablet 3438 - 12.5 '' - C...   \n341  Lenovo ThinkPad X230 2324 - 12.5 '' - Core i5 ...   \n342  Lenovo ThinkPad X230 2320 - 12.5 '' - Core i7 ...   \n\n                                             cpu_brand  \\\n0                                      Intel. i5-3320M   \n1           1.6 GHz Intel Core i5-4200U. Intel Core I5   \n2                 1.6 GHz Intel Core i5. Intel Core I5   \n3                                                 None   \n4                         1.7 GHz Core i5-3317U. Intel   \n..                                                 ...   \n338  Intel Core i5 ( 3rd Gen ) 3320M / 2.6 GHz. Int...   \n339  Intel Core i5 ( 3rd Gen ) 3320M / 2.6 GHz. Int...   \n340  Intel Core i5 ( 3rd Gen ) 3320M / 2.6 GHz. Int...   \n341  Intel Core i5 ( 3rd Gen ) 3320M / 2.6 GHz. Int...   \n342  Intel Core i7 ( 3rd Gen ) 3520M / 2.9 GHz. Int...   \n\n                                             cpu_model  \\\n0                                             i5-3320M   \n1                                                 None   \n2                                                 None   \n3                                                 None   \n4                                                 None   \n..                                                 ...   \n338  Intel Core i5 ( 3rd Gen ) 3320M / 2.6 GHz. Int...   \n339  Intel Core i5 ( 3rd Gen ) 3320M / 2.6 GHz. Int...   \n340  Intel Core i5 ( 3rd Gen ) 3320M / 2.6 GHz. Int...   \n341  Intel Core i5 ( 3rd Gen ) 3320M / 2.6 GHz. Int...   \n342  Intel Core i7 ( 3rd Gen ) 3520M / 2.9 GHz. Int...   \n\n                                              cpu_type  \\\n0                        Dual-core ( 2 Core ). Core i5   \n1                          1.6 GHz Intel Core i5-4200U   \n2                                1.6 GHz Intel Core i5   \n3                                                 None   \n4                                1.7 GHz Core i5-3317U   \n..                                                 ...   \n338  Intel Core i5 ( 3rd Gen ) 3320M / 2.6 GHz. Int...   \n339  Intel Core i5 ( 3rd Gen ) 3320M / 2.6 GHz. Int...   \n340  Intel Core i5 ( 3rd Gen ) 3320M / 2.6 GHz. Int...   \n341  Intel Core i5 ( 3rd Gen ) 3320M / 2.6 GHz. Int...   \n342  Intel Core i7 ( 3rd Gen ) 3520M / 2.9 GHz. Int...   \n\n                                         cpu_frequency  \\\n0                                             2.60 GHz   \n1                          1.6 GHz Intel Core i5-4200U   \n2                                1.6 GHz Intel Core i5   \n3                                                 None   \n4                                1.7 GHz Core i5-3317U   \n..                                                 ...   \n338  Intel Core i5 ( 3rd Gen ) 3320M / 2.6 GHz. Int...   \n339  Intel Core i5 ( 3rd Gen ) 3320M / 2.6 GHz. Int...   \n340  Intel Core i5 ( 3rd Gen ) 3320M / 2.6 GHz. Int...   \n341  Intel Core i5 ( 3rd Gen ) 3320M / 2.6 GHz. Int...   \n342  Intel Core i7 ( 3rd Gen ) 3520M / 2.9 GHz. Int...   \n\n                                          ram_capacity  \\\n0                                                 None   \n1                                     8 GB DDR3L SDRAM   \n2                                      4 GB DDR3-SDRAM   \n3                                      4 GB SDRAM DDR3   \n4                                            4 GB DDR3   \n..                                                 ...   \n338  4 GB DDR3 Slots Qty 2 Empty Slots 1 Max RAM Su...   \n339  4 GB DDR3 Slots Qty 2 Max RAM Supported 16 GB ...   \n340  Form Factor SO DIMM 204-pin Technology DDR3 SD...   \n341  Form Factor SO DIMM 204-pin Technology DDR3 SD...   \n342  Empty Slots 1 Slots Qty 2 Max RAM Supported 16...   \n\n                                              ram_type  \\\n0          DDR3 SDRAM. DDR3-1600/PC3-12800. DDR3 SDRAM   \n1                         DDR3 SDRAM. 8 GB DDR3L SDRAM   \n2                          DDR3 SDRAM. 4 GB DDR3-SDRAM   \n3                          DDR3 SDRAM. 4 GB SDRAM DDR3   \n4                                DDR3 SDRAM. 4 GB DDR3   \n..                                                 ...   \n338  4 GB DDR3 Slots Qty 2 Empty Slots 1 Max RAM Su...   \n339  4 GB DDR3 Slots Qty 2 Max RAM Supported 16 GB ...   \n340  Form Factor SO DIMM 204-pin Technology DDR3 SD...   \n341  Form Factor SO DIMM 204-pin Technology DDR3 SD...   \n342  Empty Slots 1 Slots Qty 2 Max RAM Supported 16...   \n\n                                         ram_frequency  \\\n0                                  DDR3-1600/PC3-12800   \n1                                                 None   \n2                                                 None   \n3                                                 None   \n4                                                 None   \n..                                                 ...   \n338  4 GB DDR3 Slots Qty 2 Empty Slots 1 Max RAM Su...   \n339  4 GB DDR3 Slots Qty 2 Max RAM Supported 16 GB ...   \n340  Form Factor SO DIMM 204-pin Technology DDR3 SD...   \n341  Form Factor SO DIMM 204-pin Technology DDR3 SD...   \n342  Empty Slots 1 Slots Qty 2 Max RAM Supported 16...   \n\n                                          hdd_capacity  \\\n0                                               320 GB   \n1                         500 GB mechanical_hard_drive   \n2                         500 GB mechanical_hard_drive   \n3                                               500 GB   \n4                                               256 MB   \n..                                                 ...   \n338  180 GB SSD. 180 GB SSD. Lenovo ThinkPad X230 2...   \n339  500 GB HDD / 7200 rpm. 500 GB HDD / 7200 rpm. ...   \n340  500 GB HDD / 7200 rpm. 500 GB HDD / 7200 rpm. ...   \n341  320 GB HDD / 7200 rpm. 320 GB HDD / 7200 rpm. ...   \n342  256 GB SSD - Self Encrypting Drive. 256 GB SSD...   \n\n                                          ssd_capacity           weight  \\\n0                                                 None          1.80 kg   \n1                                                 None       4.8 pounds   \n2                                                 None       5.2 pounds   \n3                                                 None       4.8 pounds   \n4                                                 None       2.9 pounds   \n..                                                 ...              ...   \n338                             180 GB SSD. 180 GB SSD  3.3 lbs 3.3 lbs   \n339       500 GB HDD / 7200 rpm. 500 GB HDD / 7200 rpm  3.3 lbs 3.3 lbs   \n340       500 GB HDD / 7200 rpm. 500 GB HDD / 7200 rpm      4 lbs 4 lbs   \n341       320 GB HDD / 7200 rpm. 320 GB HDD / 7200 rpm  3.3 lbs 3.3 lbs   \n342  256 GB SSD - Self Encrypting Drive. 256 GB SSD...  3.3 lbs 3.3 lbs   \n\n                                     dimensions  \\\n0                                          None   \n1                   15.02 x 10.08 x 0.90 inches   \n2                      15.02 x 10.08 x 1 inches   \n3                   15.18 x 0.89 x 10.16 inches   \n4                    8.80 x 0.70 x 12.80 inches   \n..                                          ...   \n338  8.1 in. 12 in x 8.1 in x 1 in. 1 in. 12 in   \n339  8.1 in. 12 in x 8.1 in x 1 in. 1 in. 12 in   \n340  9 in. 12 in x 9 in x 1.2 in. 1.2 in. 12 in   \n341  8.1 in. 12 in x 8.1 in x 1 in. 1 in. 12 in   \n342  8.1 in. 12 in x 8.1 in x 1 in. 1 in. 12 in   \n\n                                                 title  \n0    \"Lenovo Thinkpad X230 34352jf Tablet Pc - 12.5...  \n1    Amazon.com : Acer Aspire V7-582PG-6479 15.6-In...  \n2    Amazon.com : Acer Aspire E1-572-6870 15.6 Inch...  \n3    \"Amazon.com : 15.6\"\" HP 15-f009wm Amd Dual-Cor...  \n4    Amazon.com : ASUS UX31A-XB52 13.3-Inch Ultrabo...  \n..                                                 ...  \n338  \"Lenovo ThinkPad X230 2320 - 12.5\"\" - Core i5 ...  \n339  \"Lenovo ThinkPad X230 2325 - 12.5\"\" - Core i5 ...  \n340  \"Lenovo ThinkPad X230 Tablet 3438 - 12.5\"\" - C...  \n341  \"Lenovo ThinkPad X230 2324 - 12.5\"\" - Core i5 ...  \n342  \"Lenovo ThinkPad X230 2320 - 12.5\"\" - Core i7 ...  \n\n[343 rows x 14 columns]",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>instance_id</th>\n      <th>brand</th>\n      <th>cpu_brand</th>\n      <th>cpu_model</th>\n      <th>cpu_type</th>\n      <th>cpu_frequency</th>\n      <th>ram_capacity</th>\n      <th>ram_type</th>\n      <th>ram_frequency</th>\n      <th>hdd_capacity</th>\n      <th>ssd_capacity</th>\n      <th>weight</th>\n      <th>dimensions</th>\n      <th>title</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>www.softwarecity.ca//737</td>\n      <td>Lenovo</td>\n      <td>Intel. i5-3320M</td>\n      <td>i5-3320M</td>\n      <td>Dual-core ( 2 Core ). Core i5</td>\n      <td>2.60 GHz</td>\n      <td>None</td>\n      <td>DDR3 SDRAM. DDR3-1600/PC3-12800. DDR3 SDRAM</td>\n      <td>DDR3-1600/PC3-12800</td>\n      <td>320 GB</td>\n      <td>None</td>\n      <td>1.80 kg</td>\n      <td>None</td>\n      <td>\"Lenovo Thinkpad X230 34352jf Tablet Pc - 12.5...</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>www.isupplyhub.com//1256</td>\n      <td>Acer</td>\n      <td>1.6 GHz Intel Core i5-4200U. Intel Core I5</td>\n      <td>None</td>\n      <td>1.6 GHz Intel Core i5-4200U</td>\n      <td>1.6 GHz Intel Core i5-4200U</td>\n      <td>8 GB DDR3L SDRAM</td>\n      <td>DDR3 SDRAM. 8 GB DDR3L SDRAM</td>\n      <td>None</td>\n      <td>500 GB mechanical_hard_drive</td>\n      <td>None</td>\n      <td>4.8 pounds</td>\n      <td>15.02 x 10.08 x 0.90 inches</td>\n      <td>Amazon.com : Acer Aspire V7-582PG-6479 15.6-In...</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>www.isupplyhub.com//326</td>\n      <td>Acer</td>\n      <td>1.6 GHz Intel Core i5. Intel Core I5</td>\n      <td>None</td>\n      <td>1.6 GHz Intel Core i5</td>\n      <td>1.6 GHz Intel Core i5</td>\n      <td>4 GB DDR3-SDRAM</td>\n      <td>DDR3 SDRAM. 4 GB DDR3-SDRAM</td>\n      <td>None</td>\n      <td>500 GB mechanical_hard_drive</td>\n      <td>None</td>\n      <td>5.2 pounds</td>\n      <td>15.02 x 10.08 x 1 inches</td>\n      <td>Amazon.com : Acer Aspire E1-572-6870 15.6 Inch...</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>www.isupplyhub.com//821</td>\n      <td>HP</td>\n      <td>None</td>\n      <td>None</td>\n      <td>None</td>\n      <td>None</td>\n      <td>4 GB SDRAM DDR3</td>\n      <td>DDR3 SDRAM. 4 GB SDRAM DDR3</td>\n      <td>None</td>\n      <td>500 GB</td>\n      <td>None</td>\n      <td>4.8 pounds</td>\n      <td>15.18 x 0.89 x 10.16 inches</td>\n      <td>\"Amazon.com : 15.6\"\" HP 15-f009wm Amd Dual-Cor...</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>www.isupplyhub.com//157</td>\n      <td>Asus</td>\n      <td>1.7 GHz Core i5-3317U. Intel</td>\n      <td>None</td>\n      <td>1.7 GHz Core i5-3317U</td>\n      <td>1.7 GHz Core i5-3317U</td>\n      <td>4 GB DDR3</td>\n      <td>DDR3 SDRAM. 4 GB DDR3</td>\n      <td>None</td>\n      <td>256 MB</td>\n      <td>None</td>\n      <td>2.9 pounds</td>\n      <td>8.80 x 0.70 x 12.80 inches</td>\n      <td>Amazon.com : ASUS UX31A-XB52 13.3-Inch Ultrabo...</td>\n    </tr>\n    <tr>\n      <th>...</th>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n    </tr>\n    <tr>\n      <th>338</th>\n      <td>www.vology.com//873</td>\n      <td>Lenovo ThinkPad X230 2320 - 12.5 '' - Core i5 ...</td>\n      <td>Intel Core i5 ( 3rd Gen ) 3320M / 2.6 GHz. Int...</td>\n      <td>Intel Core i5 ( 3rd Gen ) 3320M / 2.6 GHz. Int...</td>\n      <td>Intel Core i5 ( 3rd Gen ) 3320M / 2.6 GHz. Int...</td>\n      <td>Intel Core i5 ( 3rd Gen ) 3320M / 2.6 GHz. Int...</td>\n      <td>4 GB DDR3 Slots Qty 2 Empty Slots 1 Max RAM Su...</td>\n      <td>4 GB DDR3 Slots Qty 2 Empty Slots 1 Max RAM Su...</td>\n      <td>4 GB DDR3 Slots Qty 2 Empty Slots 1 Max RAM Su...</td>\n      <td>180 GB SSD. 180 GB SSD. Lenovo ThinkPad X230 2...</td>\n      <td>180 GB SSD. 180 GB SSD</td>\n      <td>3.3 lbs 3.3 lbs</td>\n      <td>8.1 in. 12 in x 8.1 in x 1 in. 1 in. 12 in</td>\n      <td>\"Lenovo ThinkPad X230 2320 - 12.5\"\" - Core i5 ...</td>\n    </tr>\n    <tr>\n      <th>339</th>\n      <td>www.vology.com//823</td>\n      <td>Lenovo ThinkPad X230 2325 - 12.5 '' - Core i5 ...</td>\n      <td>Intel Core i5 ( 3rd Gen ) 3320M / 2.6 GHz. Int...</td>\n      <td>Intel Core i5 ( 3rd Gen ) 3320M / 2.6 GHz. Int...</td>\n      <td>Intel Core i5 ( 3rd Gen ) 3320M / 2.6 GHz. Int...</td>\n      <td>Intel Core i5 ( 3rd Gen ) 3320M / 2.6 GHz. Int...</td>\n      <td>4 GB DDR3 Slots Qty 2 Max RAM Supported 16 GB ...</td>\n      <td>4 GB DDR3 Slots Qty 2 Max RAM Supported 16 GB ...</td>\n      <td>4 GB DDR3 Slots Qty 2 Max RAM Supported 16 GB ...</td>\n      <td>500 GB HDD / 7200 rpm. 500 GB HDD / 7200 rpm. ...</td>\n      <td>500 GB HDD / 7200 rpm. 500 GB HDD / 7200 rpm</td>\n      <td>3.3 lbs 3.3 lbs</td>\n      <td>8.1 in. 12 in x 8.1 in x 1 in. 1 in. 12 in</td>\n      <td>\"Lenovo ThinkPad X230 2325 - 12.5\"\" - Core i5 ...</td>\n    </tr>\n    <tr>\n      <th>340</th>\n      <td>www.vology.com//2723</td>\n      <td>Lenovo ThinkPad X230 Tablet 3438 - 12.5 '' - C...</td>\n      <td>Intel Core i5 ( 3rd Gen ) 3320M / 2.6 GHz. Int...</td>\n      <td>Intel Core i5 ( 3rd Gen ) 3320M / 2.6 GHz. Int...</td>\n      <td>Intel Core i5 ( 3rd Gen ) 3320M / 2.6 GHz. Int...</td>\n      <td>Intel Core i5 ( 3rd Gen ) 3320M / 2.6 GHz. Int...</td>\n      <td>Form Factor SO DIMM 204-pin Technology DDR3 SD...</td>\n      <td>Form Factor SO DIMM 204-pin Technology DDR3 SD...</td>\n      <td>Form Factor SO DIMM 204-pin Technology DDR3 SD...</td>\n      <td>500 GB HDD / 7200 rpm. 500 GB HDD / 7200 rpm. ...</td>\n      <td>500 GB HDD / 7200 rpm. 500 GB HDD / 7200 rpm</td>\n      <td>4 lbs 4 lbs</td>\n      <td>9 in. 12 in x 9 in x 1.2 in. 1.2 in. 12 in</td>\n      <td>\"Lenovo ThinkPad X230 Tablet 3438 - 12.5\"\" - C...</td>\n    </tr>\n    <tr>\n      <th>341</th>\n      <td>www.vology.com//1349</td>\n      <td>Lenovo ThinkPad X230 2324 - 12.5 '' - Core i5 ...</td>\n      <td>Intel Core i5 ( 3rd Gen ) 3320M / 2.6 GHz. Int...</td>\n      <td>Intel Core i5 ( 3rd Gen ) 3320M / 2.6 GHz. Int...</td>\n      <td>Intel Core i5 ( 3rd Gen ) 3320M / 2.6 GHz. Int...</td>\n      <td>Intel Core i5 ( 3rd Gen ) 3320M / 2.6 GHz. Int...</td>\n      <td>Form Factor SO DIMM 204-pin Technology DDR3 SD...</td>\n      <td>Form Factor SO DIMM 204-pin Technology DDR3 SD...</td>\n      <td>Form Factor SO DIMM 204-pin Technology DDR3 SD...</td>\n      <td>320 GB HDD / 7200 rpm. 320 GB HDD / 7200 rpm. ...</td>\n      <td>320 GB HDD / 7200 rpm. 320 GB HDD / 7200 rpm</td>\n      <td>3.3 lbs 3.3 lbs</td>\n      <td>8.1 in. 12 in x 8.1 in x 1 in. 1 in. 12 in</td>\n      <td>\"Lenovo ThinkPad X230 2324 - 12.5\"\" - Core i5 ...</td>\n    </tr>\n    <tr>\n      <th>342</th>\n      <td>www.vology.com//3017</td>\n      <td>Lenovo ThinkPad X230 2320 - 12.5 '' - Core i7 ...</td>\n      <td>Intel Core i7 ( 3rd Gen ) 3520M / 2.9 GHz. Int...</td>\n      <td>Intel Core i7 ( 3rd Gen ) 3520M / 2.9 GHz. Int...</td>\n      <td>Intel Core i7 ( 3rd Gen ) 3520M / 2.9 GHz. Int...</td>\n      <td>Intel Core i7 ( 3rd Gen ) 3520M / 2.9 GHz. Int...</td>\n      <td>Empty Slots 1 Slots Qty 2 Max RAM Supported 16...</td>\n      <td>Empty Slots 1 Slots Qty 2 Max RAM Supported 16...</td>\n      <td>Empty Slots 1 Slots Qty 2 Max RAM Supported 16...</td>\n      <td>256 GB SSD - Self Encrypting Drive. 256 GB SSD...</td>\n      <td>256 GB SSD - Self Encrypting Drive. 256 GB SSD...</td>\n      <td>3.3 lbs 3.3 lbs</td>\n      <td>8.1 in. 12 in x 8.1 in x 1 in. 1 in. 12 in</td>\n      <td>\"Lenovo ThinkPad X230 2320 - 12.5\"\" - Core i7 ...</td>\n    </tr>\n  </tbody>\n</table>\n<p>343 rows × 14 columns</p>\n</div>"
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = spark.read.csv(\"data/X2.csv\", header=True)\n",
    "df.toPandas()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "# 0. Data cleaning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "for c in df.columns:\n",
    "#set everything to lowercase\n",
    "    df = df.withColumn(c, f.lower(f.col(c)))\n",
    "\n",
    "#extract brand or infer from title\n",
    "df = df.drop('ssd_capacity')\n",
    "df = df.withColumn('brand', f.regexp_extract('brand', \"^(\\w+)\", 0))\n",
    "computer_brands = ['(lenovo', 'acer', 'hp', 'dell', 'asus', 'samsung', 'huawei', 'surface', 'apple)']\n",
    "computer_brands_pattern = '|'.join(computer_brands)\n",
    "df = df.withColumn('brand', f.when( f.regexp_extract('title', computer_brands_pattern, 0)!='', f.regexp_extract('title', computer_brands_pattern, 0))\\\n",
    "                   .otherwise(df.brand))\n",
    "#exctract cpu_brand and infer type if intel\n",
    "cpu_brands = ['(intel', 'apple', 'amd', 'nvidia', 'arm)']\n",
    "cpu_pattern = '|'.join(cpu_brands)\n",
    "df = df.withColumn('cpu_model',f.regexp_extract('cpu_model', '(i\\d|pentium|celeron|a\\d)', 0))\n",
    "df = df.withColumn('cpu_model', f.when( (f.regexp_extract('cpu_brand','(intel|amd)', 0 )!='') & f.isnull(df.cpu_model) ,\\\n",
    "                                        f.regexp_extract('cpu_brand', '(i\\d|pentium|celeron|a\\d)', 0))\\\n",
    "                   .otherwise(df.cpu_model))\n",
    "df = df.withColumn('cpu_brand', f.when(f.regexp_extract('cpu_brand', cpu_pattern, 0) != '', f.regexp_extract('cpu_brand', cpu_pattern, 1))\\\n",
    "                                       .otherwise(f.regexp_extract('title', cpu_pattern, 0)))\n",
    "df = df.withColumn('weight', f.when(df.weight.contains('pounds') | df.weight.contains('lbs'),\n",
    "                                    (f.regexp_extract('weight', '(\\d+.?\\d)', 0).cast(t.DoubleType()))).otherwise(\n",
    "                                    f.round(f.regexp_extract('weight', '(\\d+.?\\d)', 0).cast(t.DoubleType())*2.20462,1)\n",
    "                        )\n",
    "                    )\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Blocking\n",
    "Blocking will be done feeding a TF-IDF matrix to an LDA model and extracting\n",
    "keywords from the title matching them to topics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "\"\"\"UTILITIES\"\"\"\n",
    "\n",
    "\"\"\"Returns the df with tokenized columns with stopwords removed\"\"\"\n",
    "def tokenize(df, string_cols):\n",
    "  output = df\n",
    "  stopW = ['softwarecity', 'amazon', 'com','pc', 'windows', 'computers', 'computer', 'accessories', 'laptop', 'notebook', 'kg', 'inch', 'processor', 'memory','gb', 'ram', 'hdd', 'ssd', 'cpu', 'display', 'hz', 'ghz', 'tb','rpm', 'slot', 'slots', 'mhz', 'cache', 'ram', 'ddram', 'dram', 'hd']\n",
    "  for c in string_cols:\n",
    "    output = output.withColumn('temp', f.coalesce(f.col(c), f.lower(c), f.lit('')))\n",
    "    tokenizer = RegexTokenizer(inputCol='temp', outputCol=c+\"_tokens\", pattern = \"\\\\W\")\n",
    "    remover = StopWordsRemover(inputCol=c+\"_tokens\", outputCol=c+\"_swRemoved\", stopWords=stopW)\n",
    "    output = tokenizer.transform(output)\n",
    "\n",
    "    filter_alnum = f.udf(lambda l : [t for t in l if t.isalpha() and len(t) >= 2], t.ArrayType(t.StringType()))\n",
    "    output = output.withColumn(c+'_tokens', filter_alnum(f.col(c+\"_tokens\")))\n",
    "\n",
    "    output = remover.transform(output)\\\n",
    "      .drop('temp', c+\"_tokens\")\n",
    "    # output has c+swRemoved columns\n",
    "  return output\n",
    "\n",
    "def generate_blocking_keys(df, token_cols, min_freq=1):\n",
    "    \"\"\"Pipeline:\n",
    "            1 - CountVectorizer -> TF\n",
    "            2 - IDF\n",
    "            3 - LDA\n",
    "    \"\"\"\n",
    "    df = df.withColumn('tokens_swRemoved', f.concat(*token_cols))\n",
    "    cv = CountVectorizer(inputCol='tokens_swRemoved', outputCol=\"rawFeatures\")\n",
    "    cvmodel = cv.fit(df)\n",
    "    df_vect = cvmodel.transform(df)\n",
    "\n",
    "    idf = IDF(inputCol=\"rawFeatures\", outputCol=\"features\", minDocFreq=min_freq)\n",
    "    idfModel = idf.fit(df_vect)\n",
    "    df_idf= idfModel.transform(df_vect)\n",
    "\n",
    "    normalizer = Normalizer(p=2.0, inputCol='features', outputCol='tfidf')\n",
    "    output = normalizer.transform(df_idf)\n",
    "\n",
    "    lda = LDA(k=5, maxIter=1000, featuresCol='tfidf')\n",
    "    lda_model = lda.fit(output)\n",
    "    vocab = cvmodel.vocabulary\n",
    "    #returns words for each topic term\n",
    "    def get_words(token_list):\n",
    "        return [vocab[token_id] for token_id in token_list]\n",
    "\n",
    "    udf_to_words = f.udf(get_words, t.ArrayType(t.StringType()))\n",
    "\n",
    "    #create list of topic keywords\n",
    "    # i.e topic 1 -> acer, anspire, intel\n",
    "    topics = lda_model.describeTopics(3).withColumn('topicWords', udf_to_words(f.col('termIndices'))).collect()\n",
    "    list_of_topics = []\n",
    "    for r in topics:\n",
    "        topicW = r.__getitem__('topicWords')\n",
    "        for w in topicW:\n",
    "            list_of_topics.append(w)\n",
    "\n",
    "    #returns list of 3 'hashtags' i.e keywords for topic\n",
    "    #from tokens: title, brand, cpu_brand\n",
    "    def get_key(words):\n",
    "        l = [w for w in words if w in list_of_topics]\n",
    "        l = list(set(l))\n",
    "        l.sort()\n",
    "        return l[:3]\n",
    "    udf_get_key = f.udf(get_key, t.ArrayType(t.StringType()))\n",
    "    output = output.withColumn(\"blocking_key\", udf_get_key(f.col(\"tokens_swRemoved\")))\n",
    "    output.select(\"blocking_key\").show()\n",
    "    return output\n",
    "\n",
    "\"\"\"Use universal sentence encoder from tensorflow_hub\"\"\"\n",
    "MODEL = None\n",
    "def get_model_magic():\n",
    "  global MODEL\n",
    "  if MODEL is None:\n",
    "      MODEL = hub.load(\"https://tfhub.dev/google/universal-sentence-encoder/4\")\n",
    "  return MODEL\n",
    "\n",
    "@f.udf(returnType=VectorUDT())\n",
    "def encode_sentence(x):\n",
    "  model = get_model_magic()\n",
    "  emb = model([x]).numpy()[0]\n",
    "  return Vectors.dense(emb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "columns = ['title', 'brand', 'cpu_brand', 'cpu_model', 'ram_type', 'ram_capacity', 'hdd_capacity', 'weight']\n",
    "blocking_df = tokenize(df, columns[:2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+\n",
      "|        blocking_key|\n",
      "+--------------------+\n",
      "|[lenovo, professi...|\n",
      "|      [acer, aspire]|\n",
      "|[acer, aspire, dr...|\n",
      "|                [hp]|\n",
      "|[aluminum, asus, ...|\n",
      "|[drive, lenovo, t...|\n",
      "|      [acer, aspire]|\n",
      "|      [acer, aspire]|\n",
      "|[drive, lenovo, t...|\n",
      "|      [acer, aspire]|\n",
      "|    [dell, inspiron]|\n",
      "|[dell, drive, ins...|\n",
      "|    [dell, inspiron]|\n",
      "|      [acer, aspire]|\n",
      "|      [acer, aspire]|\n",
      "|      [acer, aspire]|\n",
      "|[lenovo, professi...|\n",
      "|    [dell, inspiron]|\n",
      "|      [acer, aspire]|\n",
      "|      [acer, aspire]|\n",
      "+--------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#Generate Blocking Keys\n",
    "#for c in columns:\n",
    "    #blocking_df = blocking_df.withColumn(c+'_encoding', encode_sentence(f.coalesce(f.col(c), f.lit(''))))\n",
    "blocking_df = generate_blocking_keys(blocking_df,\n",
    "                                     [c+'_swRemoved' for c in columns[:2]])\n",
    "\n",
    "#Add encoding on title\n",
    "blocking_df = blocking_df.withColumn('title_encoding', encode_sentence(f.coalesce(f.col('title'), f.lit(''))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+-----+\n",
      "|        blocking_key|count|\n",
      "+--------------------+-----+\n",
      "|[dell, drive, ins...|    4|\n",
      "|[acer, aspire, pr...|   22|\n",
      "|    [aluminum, asus]|    1|\n",
      "|[acer, aspire, dr...|    7|\n",
      "|[elitebook, hp, pro]|    4|\n",
      "|[lenovo, professi...|    3|\n",
      "|      [acer, aspire]|   32|\n",
      "|[elitebook, hp, p...|    2|\n",
      "|[drive, lenovo, t...|    2|\n",
      "|[elitebook, folio...|    8|\n",
      "|[lenovo, pro, thi...|  220|\n",
      "|         [drive, hp]|    1|\n",
      "|[aluminum, asus, ...|    1|\n",
      "|                [hp]|    7|\n",
      "|       [lenovo, pro]|    1|\n",
      "|    [dell, inspiron]|    5|\n",
      "|[lenovo, professi...|    1|\n",
      "|[lenovo, premium,...|    3|\n",
      "|  [lenovo, thinkpad]|    4|\n",
      "|     [elitebook, hp]|   15|\n",
      "+--------------------+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "blocking_df.groupby('blocking_key').count().show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "# 2. Candidate pairs generation and match likelihood"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+\n",
      "|      title_encoding|\n",
      "+--------------------+\n",
      "|[0.01956569217145...|\n",
      "+--------------------+\n",
      "\n",
      "['instance_id', 'brand', 'cpu_brand', 'cpu_model', 'cpu_type', 'cpu_frequency', 'ram_capacity', 'ram_type', 'ram_frequency', 'hdd_capacity', 'weight', 'dimensions', 'title', 'title_swRemoved', 'brand_swRemoved', 'tokens_swRemoved', 'rawFeatures', 'features', 'tfidf', 'blocking_key', 'title_encoding']\n",
      "+--------------------+--------------------+\n",
      "|        blocking_key|                  id|\n",
      "+--------------------+--------------------+\n",
      "|[dell, drive, ins...|www.isupplyhub.co...|\n",
      "|[dell, drive, ins...|www.amazon.com//1835|\n",
      "|[dell, drive, ins...|   www.amazon.com//8|\n",
      "|[dell, drive, ins...|www.isupplyhub.co...|\n",
      "|[acer, aspire, pr...|       buy.net//1992|\n",
      "|[acer, aspire, pr...|www.flexshopper.c...|\n",
      "|[acer, aspire, pr...|       buy.net//2012|\n",
      "|[acer, aspire, pr...|        buy.net//634|\n",
      "|[acer, aspire, pr...|www.flexshopper.c...|\n",
      "|[acer, aspire, pr...|www.flexshopper.c...|\n",
      "|[acer, aspire, pr...|www.flexshopper.c...|\n",
      "|[acer, aspire, pr...|www.flexshopper.c...|\n",
      "|[acer, aspire, pr...|www.amazon.com//1664|\n",
      "|[acer, aspire, pr...|www.amazon.com//1081|\n",
      "|[acer, aspire, pr...|www.flexshopper.c...|\n",
      "|[acer, aspire, pr...|www.flexshopper.c...|\n",
      "|[acer, aspire, pr...|www.amazon.com//2395|\n",
      "|[acer, aspire, pr...|www.flexshopper.c...|\n",
      "|[acer, aspire, pr...|       buy.net//1759|\n",
      "|[acer, aspire, pr...|www.amazon.com//1313|\n",
      "+--------------------+--------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "This cell output a candidates dataframe that has\n",
    "instance_ids pairs that makes sense to compare, i.e each\n",
    "entity will be paired with another entity from the same block\n",
    "\"\"\"\n",
    "#cols_to_keep = [c+'_encoding' for c in columns]\n",
    "#for c in columns:\n",
    "#    cols_to_keep.append(c)\n",
    "#cols_to_keep.append('tokens_swRemoved')\n",
    "#cols_to_keep.append('tfidf')\n",
    "#cols_to_keep.append('instance_id')\n",
    "cols_to_keep = ['instance_id', 'title_encoding']\n",
    "#node = blocking_df.select(f.col('instance_id').alias('id'), *cols_to_keep).drop('instance_id')\n",
    "node = blocking_df.select(f.col('instance_id').alias('id'), 'title_encoding').drop('instance_id')\n",
    "node.select('title_encoding').limit(1).show()\n",
    "print(blocking_df.columns)\n",
    "pairs = blocking_df.select(*cols_to_keep, 'blocking_key')\\\n",
    "    .groupby('blocking_key').agg(f.count('instance_id').alias('size'), f.collect_set('instance_id').alias('id'))\\\n",
    "    .filter(f.col('size') > 1).select('blocking_key',f.explode('id').alias('id'))\n",
    "pairs.show()\n",
    "\n",
    "left = pairs.withColumnRenamed('id', 'src')\n",
    "right = pairs.withColumnRenamed('id', 'dst')\n",
    "#candidates based on matching of blocking_key (i.e inside the block)\n",
    "candidates = left.join(right, ['blocking_key'], 'inner')\\\n",
    "    .filter(f.col('src') < f.col('dst'))\\\n",
    "    .select('src', 'dst').distinct()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": "\"\\n@f.udf(returnType=t.DoubleType())\\ndef dot(x, y):\\n  if x is not None and y is not None:\\n    return float(x.dot(y))\\n  else:\\n    return 0\\n\\ndef null_safe_levenshtein_sim(c1, c2):\\n  output = f.when(f.col(c1).isNull() | f.col(c2).isNull(), 0)            .otherwise(1 - f.levenshtein(c1, c2) / f.greatest(f.length(c1), f.length(c2)))\\n  return output\\n\\ndef null_safe_num_sim(c1, c2):\\n  output = f.when(f.col(c1).isNull() | f.col(c2).isNull(), 0)            .when((f.col(c1) == 0) & (f.col(c2) == 0), 1)            .when((f.col(c1) == 0) | (f.col(c2) == 0), 0)            .otherwise(1 - f.abs(f.col(c1) - f.col(c2)) / f.greatest(c1, c2))\\n  return output\\n\\ndef null_safe_token_overlap(c1, c2):\\n  # is the overlap a significant part of the shorter string\\n  output = f.when(f.col(c1).isNull() | f.col(c2).isNull(), 0)            .when((f.size(f.array_distinct(c1)) == 0) | (f.size(f.array_distinct(c2)) == 0), 0)            .otherwise(f.size(f.array_intersect(c1, c2)) / f.least(f.size(f.array_distinct(c1)), f.size(f.array_distinct(c1))))\\n  return output\\n\\ndef calc_sim(df, candidates):\\n    metrics = []\\n    for c in columns[:2]:\\n        if '_encoding' not in c:\\n            candidates = candidates.withColumn(c+'_lev', null_safe_levenshtein_sim(df.filter(df.id == candidates.src).select(c),df.filter(df.id == candidates.dst).select(c)))\\n            metrics.append(c+'_lev')\\n        else:\\n            metrics.append(c+'_sim')\\n            candidates = candidates.withColumn(c+'_sim', dot(df.filter(df.id == candidates.src).select(c), df.filter(df.id == candidates.dst).select(c)))\\n    candidates = candidates.withColumn('tfidf_sim', dot(df.filter(df.id == candidates.src).select('tfidf'),df.filter(df.id == candidates.dst).select('tfidf')))\\n    candidates = candidates.withColumn('token_sim', dot(df.filter(df.id == candidates.src).select('tokens_swRemoved'), df.filter(df.id == candidates.dst).select('tokens_swRemoved')))\\n    candidates = candidates.withColumn('weight_sim', dot(df.filter(df.id == candidates.src).select('weight'),df.filter(df.id == candidates.dst).select('weight')))\\n    metrics.append('tfidf_sim')\\n    metrics.append('token_sim')\\n    metrics.append('weigth_sim')\\n    def sum_distance(distances):\\n        return sum(d for d in distances)\\n    udf_sum = f.udf(sum_distance, t.DoubleType())\\n    candidates = candidates.withColumn('sum_sim', udf_sum([f.col(c) for c in metrics]))\\n    udf_norm = f.udf(lambda d : d / len(metrics))\\n    candidates = candidates.withColumn('overall_sim', udf_norm(f.col('sum_sim'))).drop(f.col('sum_sim'))\\n    return candidates\\n\\ndistance_df = calc_sim(node, candidates)\\n\""
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "@f.udf(returnType=t.DoubleType())\n",
    "def dot(x, y):\n",
    "  if x is not None and y is not None:\n",
    "    return float(x.dot(y))\n",
    "  else:\n",
    "    return 0\n",
    "\n",
    "def null_safe_levenshtein_sim(c1, c2):\n",
    "  output = f.when(f.col(c1).isNull() | f.col(c2).isNull(), 0)\\\n",
    "            .otherwise(1 - f.levenshtein(c1, c2) / f.greatest(f.length(c1), f.length(c2)))\n",
    "  return output\n",
    "\n",
    "def null_safe_num_sim(c1, c2):\n",
    "  output = f.when(f.col(c1).isNull() | f.col(c2).isNull(), 0)\\\n",
    "            .when((f.col(c1) == 0) & (f.col(c2) == 0), 1)\\\n",
    "            .when((f.col(c1) == 0) | (f.col(c2) == 0), 0)\\\n",
    "            .otherwise(1 - f.abs(f.col(c1) - f.col(c2)) / f.greatest(c1, c2))\n",
    "  return output\n",
    "\n",
    "def null_safe_token_overlap(c1, c2):\n",
    "  # is the overlap a significant part of the shorter string\n",
    "  output = f.when(f.col(c1).isNull() | f.col(c2).isNull(), 0)\\\n",
    "            .when((f.size(f.array_distinct(c1)) == 0) | (f.size(f.array_distinct(c2)) == 0), 0)\\\n",
    "            .otherwise(f.size(f.array_intersect(c1, c2)) / f.least(f.size(f.array_distinct(c1)), f.size(f.array_distinct(c1))))\n",
    "  return output\n",
    "\n",
    "def calc_sim(df, candidates):\n",
    "    metrics = []\n",
    "    for c in columns[:2]:\n",
    "        if '_encoding' not in c:\n",
    "            candidates = candidates.withColumn(c+'_lev', null_safe_levenshtein_sim(df.filter(df.id == candidates.src).select(c),df.filter(df.id == candidates.dst).select(c)))\n",
    "            metrics.append(c+'_lev')\n",
    "        else:\n",
    "            metrics.append(c+'_sim')\n",
    "            candidates = candidates.withColumn(c+'_sim', dot(df.filter(df.id == candidates.src).select(c), df.filter(df.id == candidates.dst).select(c)))\n",
    "    candidates = candidates.withColumn('tfidf_sim', dot(df.filter(df.id == candidates.src).select('tfidf'),df.filter(df.id == candidates.dst).select('tfidf')))\n",
    "    candidates = candidates.withColumn('token_sim', dot(df.filter(df.id == candidates.src).select('tokens_swRemoved'), df.filter(df.id == candidates.dst).select('tokens_swRemoved')))\n",
    "    candidates = candidates.withColumn('weight_sim', dot(df.filter(df.id == candidates.src).select('weight'),df.filter(df.id == candidates.dst).select('weight')))\n",
    "    metrics.append('tfidf_sim')\n",
    "    metrics.append('token_sim')\n",
    "    metrics.append('weigth_sim')\n",
    "    def sum_distance(distances):\n",
    "        return sum(d for d in distances)\n",
    "    udf_sum = f.udf(sum_distance, t.DoubleType())\n",
    "    candidates = candidates.withColumn('sum_sim', udf_sum([f.col(c) for c in metrics]))\n",
    "    udf_norm = f.udf(lambda d : d / len(metrics))\n",
    "    candidates = candidates.withColumn('overall_sim', udf_norm(f.col('sum_sim'))).drop(f.col('sum_sim'))\n",
    "    return candidates\n",
    "\n",
    "distance_df = calc_sim(node, candidates)\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "60936\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "Read label.csv and expand it trough transitivity\n",
    "\"\"\"\n",
    "import csv\n",
    "dictionary = {}\n",
    "def countNum(row):\n",
    "    # <A,B>\n",
    "   if row.lid in dictionary.keys():\n",
    "       dictionary[row.lid].add(row.rid) # A -> (A,B,...)\n",
    "   else:\n",
    "       dictionary[row.lid] = set([row.rid, row.lid]) # create set\n",
    "\n",
    "   if row.rid in dictionary.keys():\n",
    "       dictionary[row.rid].add(row.lid) # B -> (A,B,...)\n",
    "   else:\n",
    "       dictionary[row.rid] = set([row.lid, row.rid])\n",
    "\n",
    "\n",
    "labels = spark.read.csv(\"data/Y2.csv\", header=True).withColumnRenamed('left_instance_id', 'lid').withColumnRenamed('right_instance_id', 'rid')\n",
    "filter_label = labels.filter(labels.label == '1')\n",
    "collected_filtered = filter_label.collect()\n",
    "\n",
    "for row in collected_filtered:\n",
    "    countNum(row)\n",
    "file = open('./data/tmp.csv', 'w+')\n",
    "csv_writer = csv.writer(file)\n",
    "csv_writer.writerow(['lid','rid','label'])\n",
    "for key in dictionary.keys():\n",
    "    elem_set = dictionary[key]\n",
    "    while len(elem_set) > 0:\n",
    "        first_elem = elem_set.pop()\n",
    "        for second_elem in elem_set:\n",
    "            if first_elem != second_elem:\n",
    "                l = [first_elem, second_elem, '1']\n",
    "                csv_writer.writerow(l)\n",
    "                if key == first_elem:\n",
    "                    continue\n",
    "                if first_elem in dictionary.keys() and second_elem in dictionary[first_elem]:\n",
    "                    dictionary[first_elem].remove(second_elem)\n",
    "file.close()\n",
    "updated_labels = spark.read.csv(\"data/tmp.csv\", header=True)\n",
    "updated_labels = updated_labels.distinct()\n",
    "labels = labels.union(updated_labels)\n",
    "print(labels.count())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['lid', 'rid', 'label', 'l_title_encoding', 'r_title_encoding']\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "Reuse this cell to join a <left_id,right_id> with node to extract features\n",
    "\"\"\"\n",
    "#label_df = labels.join(candidates.withColumnRenamed('src','lid').withColumnRenamed('dst','rid'), ['lid','rid'], 'inner')\n",
    "label_df = labels.join(node.alias(\"node_1\"), labels.lid == node.id, 'inner').drop('id')\n",
    "for c in cols_to_keep[1:]:\n",
    "    label_df = label_df.withColumnRenamed(c, 'l_'+c)\n",
    "\n",
    "label_df = label_df.alias('one').join(node.alias(\"node_2\"), label_df.rid == node.id, 'inner').drop('id')\n",
    "for c in cols_to_keep[1:]:\n",
    "    label_df = label_df.withColumnRenamed(c, 'r_'+c)\n",
    "print(label_df.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def toList(row):\n",
    "    l = []\n",
    "    for v in row:\n",
    "        for n in v:\n",
    "            l.append(float(n))\n",
    "    return Vectors.dense(l)\n",
    "\n",
    "udf_toList = f.udf(toList, VectorUDT())\n",
    "label_df = label_df.withColumn('features', udf_toList(f.array('l_title_encoding', 'r_title_encoding')))\\\n",
    "    .drop('l_title_encoding', 'r_title_encoding')\n",
    "label_df = label_df.withColumn('label', f.col('label').cast(t.IntegerType()))\n",
    "label_df = label_df.withColumn('weights', f.when(f.col('label')==0, 0.66).otherwise(1.0))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Machine Learning Magic Bitch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.ml.classification import LinearSVC, LogisticRegression\n",
    "from pyspark.ml.tuning import ParamGridBuilder, CrossValidator\n",
    "from pyspark.ml.evaluation import BinaryClassificationEvaluator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "model = LinearSVC(featuresCol='features', labelCol='label', weightCol='weights',maxIter=100)\n",
    "param_grid = ParamGridBuilder().addGrid(model.regParam, [0.5, 0.4, 0.3, 0.2, 0.1]).build()\n",
    "cvs = CrossValidator(estimator=model,\n",
    "                           estimatorParamMaps=param_grid,\n",
    "                           evaluator=BinaryClassificationEvaluator(),#(rawPredictionCol='prediction', labelCol='label'),\\\n",
    "                           numFolds=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "training_set, test_set = label_df.randomSplit([0.8, 0.2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "ename": "Py4JJavaError",
     "evalue": "An error occurred while calling o7215.fit.\n: java.util.concurrent.ExecutionException: org.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 33556.0 failed 1 times, most recent failure: Lost task 0.0 in stage 33556.0 (TID 2253059) (192.168.1.51 executor driver): TaskResultLost (result lost from block manager)\nDriver stacktrace:\n\tat java.base/java.util.concurrent.FutureTask.report(FutureTask.java:122)\n\tat java.base/java.util.concurrent.FutureTask.get(FutureTask.java:205)\n\tat org.apache.spark.sql.execution.exchange.BroadcastExchangeExec.doExecuteBroadcast(BroadcastExchangeExec.scala:194)\n\tat org.apache.spark.sql.execution.InputAdapter.doExecuteBroadcast(WholeStageCodegenExec.scala:515)\n\tat org.apache.spark.sql.execution.SparkPlan.$anonfun$executeBroadcast$1(SparkPlan.scala:193)\n\tat org.apache.spark.sql.execution.SparkPlan.$anonfun$executeQuery$1(SparkPlan.scala:218)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n\tat org.apache.spark.sql.execution.SparkPlan.executeQuery(SparkPlan.scala:215)\n\tat org.apache.spark.sql.execution.SparkPlan.executeBroadcast(SparkPlan.scala:189)\n\tat org.apache.spark.sql.execution.joins.BroadcastHashJoinExec.prepareBroadcast(BroadcastHashJoinExec.scala:203)\n\tat org.apache.spark.sql.execution.joins.BroadcastHashJoinExec.prepareRelation(BroadcastHashJoinExec.scala:217)\n\tat org.apache.spark.sql.execution.joins.HashJoin.codegenInner(HashJoin.scala:443)\n\tat org.apache.spark.sql.execution.joins.HashJoin.codegenInner$(HashJoin.scala:442)\n\tat org.apache.spark.sql.execution.joins.BroadcastHashJoinExec.codegenInner(BroadcastHashJoinExec.scala:40)\n\tat org.apache.spark.sql.execution.joins.HashJoin.doConsume(HashJoin.scala:351)\n\tat org.apache.spark.sql.execution.joins.HashJoin.doConsume$(HashJoin.scala:349)\n\tat org.apache.spark.sql.execution.joins.BroadcastHashJoinExec.doConsume(BroadcastHashJoinExec.scala:40)\n\tat org.apache.spark.sql.execution.CodegenSupport.consume(WholeStageCodegenExec.scala:194)\n\tat org.apache.spark.sql.execution.CodegenSupport.consume$(WholeStageCodegenExec.scala:149)\n\tat org.apache.spark.sql.execution.InputAdapter.consume(WholeStageCodegenExec.scala:496)\n\tat org.apache.spark.sql.execution.InputRDDCodegen.doProduce(WholeStageCodegenExec.scala:483)\n\tat org.apache.spark.sql.execution.InputRDDCodegen.doProduce$(WholeStageCodegenExec.scala:456)\n\tat org.apache.spark.sql.execution.InputAdapter.doProduce(WholeStageCodegenExec.scala:496)\n\tat org.apache.spark.sql.execution.CodegenSupport.$anonfun$produce$1(WholeStageCodegenExec.scala:95)\n\tat org.apache.spark.sql.execution.SparkPlan.$anonfun$executeQuery$1(SparkPlan.scala:218)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n\tat org.apache.spark.sql.execution.SparkPlan.executeQuery(SparkPlan.scala:215)\n\tat org.apache.spark.sql.execution.CodegenSupport.produce(WholeStageCodegenExec.scala:90)\n\tat org.apache.spark.sql.execution.CodegenSupport.produce$(WholeStageCodegenExec.scala:90)\n\tat org.apache.spark.sql.execution.InputAdapter.produce(WholeStageCodegenExec.scala:496)\n\tat org.apache.spark.sql.execution.joins.HashJoin.doProduce(HashJoin.scala:346)\n\tat org.apache.spark.sql.execution.joins.HashJoin.doProduce$(HashJoin.scala:345)\n\tat org.apache.spark.sql.execution.joins.BroadcastHashJoinExec.doProduce(BroadcastHashJoinExec.scala:40)\n\tat org.apache.spark.sql.execution.CodegenSupport.$anonfun$produce$1(WholeStageCodegenExec.scala:95)\n\tat org.apache.spark.sql.execution.SparkPlan.$anonfun$executeQuery$1(SparkPlan.scala:218)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n\tat org.apache.spark.sql.execution.SparkPlan.executeQuery(SparkPlan.scala:215)\n\tat org.apache.spark.sql.execution.CodegenSupport.produce(WholeStageCodegenExec.scala:90)\n\tat org.apache.spark.sql.execution.CodegenSupport.produce$(WholeStageCodegenExec.scala:90)\n\tat org.apache.spark.sql.execution.joins.BroadcastHashJoinExec.produce(BroadcastHashJoinExec.scala:40)\n\tat org.apache.spark.sql.execution.ProjectExec.doProduce(basicPhysicalOperators.scala:54)\n\tat org.apache.spark.sql.execution.CodegenSupport.$anonfun$produce$1(WholeStageCodegenExec.scala:95)\n\tat org.apache.spark.sql.execution.SparkPlan.$anonfun$executeQuery$1(SparkPlan.scala:218)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n\tat org.apache.spark.sql.execution.SparkPlan.executeQuery(SparkPlan.scala:215)\n\tat org.apache.spark.sql.execution.CodegenSupport.produce(WholeStageCodegenExec.scala:90)\n\tat org.apache.spark.sql.execution.CodegenSupport.produce$(WholeStageCodegenExec.scala:90)\n\tat org.apache.spark.sql.execution.ProjectExec.produce(basicPhysicalOperators.scala:41)\n\tat org.apache.spark.sql.execution.joins.HashJoin.doProduce(HashJoin.scala:346)\n\tat org.apache.spark.sql.execution.joins.HashJoin.doProduce$(HashJoin.scala:345)\n\tat org.apache.spark.sql.execution.joins.BroadcastHashJoinExec.doProduce(BroadcastHashJoinExec.scala:40)\n\tat org.apache.spark.sql.execution.CodegenSupport.$anonfun$produce$1(WholeStageCodegenExec.scala:95)\n\tat org.apache.spark.sql.execution.SparkPlan.$anonfun$executeQuery$1(SparkPlan.scala:218)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n\tat org.apache.spark.sql.execution.SparkPlan.executeQuery(SparkPlan.scala:215)\n\tat org.apache.spark.sql.execution.CodegenSupport.produce(WholeStageCodegenExec.scala:90)\n\tat org.apache.spark.sql.execution.CodegenSupport.produce$(WholeStageCodegenExec.scala:90)\n\tat org.apache.spark.sql.execution.joins.BroadcastHashJoinExec.produce(BroadcastHashJoinExec.scala:40)\n\tat org.apache.spark.sql.execution.ProjectExec.doProduce(basicPhysicalOperators.scala:54)\n\tat org.apache.spark.sql.execution.CodegenSupport.$anonfun$produce$1(WholeStageCodegenExec.scala:95)\n\tat org.apache.spark.sql.execution.SparkPlan.$anonfun$executeQuery$1(SparkPlan.scala:218)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n\tat org.apache.spark.sql.execution.SparkPlan.executeQuery(SparkPlan.scala:215)\n\tat org.apache.spark.sql.execution.CodegenSupport.produce(WholeStageCodegenExec.scala:90)\n\tat org.apache.spark.sql.execution.CodegenSupport.produce$(WholeStageCodegenExec.scala:90)\n\tat org.apache.spark.sql.execution.ProjectExec.produce(basicPhysicalOperators.scala:41)\n\tat org.apache.spark.sql.execution.WholeStageCodegenExec.doCodeGen(WholeStageCodegenExec.scala:655)\n\tat org.apache.spark.sql.execution.WholeStageCodegenExec.doExecute(WholeStageCodegenExec.scala:718)\n\tat org.apache.spark.sql.execution.SparkPlan.$anonfun$execute$1(SparkPlan.scala:180)\n\tat org.apache.spark.sql.execution.SparkPlan.$anonfun$executeQuery$1(SparkPlan.scala:218)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n\tat org.apache.spark.sql.execution.SparkPlan.executeQuery(SparkPlan.scala:215)\n\tat org.apache.spark.sql.execution.SparkPlan.execute(SparkPlan.scala:176)\n\tat org.apache.spark.sql.execution.python.EvalPythonExec.doExecute(EvalPythonExec.scala:88)\n\tat org.apache.spark.sql.execution.python.EvalPythonExec.doExecute$(EvalPythonExec.scala:87)\n\tat org.apache.spark.sql.execution.python.BatchEvalPythonExec.doExecute(BatchEvalPythonExec.scala:34)\n\tat org.apache.spark.sql.execution.SparkPlan.$anonfun$execute$1(SparkPlan.scala:180)\n\tat org.apache.spark.sql.execution.SparkPlan.$anonfun$executeQuery$1(SparkPlan.scala:218)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n\tat org.apache.spark.sql.execution.SparkPlan.executeQuery(SparkPlan.scala:215)\n\tat org.apache.spark.sql.execution.SparkPlan.execute(SparkPlan.scala:176)\n\tat org.apache.spark.sql.execution.InputAdapter.inputRDD(WholeStageCodegenExec.scala:525)\n\tat org.apache.spark.sql.execution.InputRDDCodegen.inputRDDs(WholeStageCodegenExec.scala:453)\n\tat org.apache.spark.sql.execution.InputRDDCodegen.inputRDDs$(WholeStageCodegenExec.scala:452)\n\tat org.apache.spark.sql.execution.InputAdapter.inputRDDs(WholeStageCodegenExec.scala:496)\n\tat org.apache.spark.sql.execution.ProjectExec.inputRDDs(basicPhysicalOperators.scala:50)\n\tat org.apache.spark.sql.execution.SortExec.inputRDDs(SortExec.scala:132)\n\tat org.apache.spark.sql.execution.SampleExec.inputRDDs(basicPhysicalOperators.scala:311)\n\tat org.apache.spark.sql.execution.ProjectExec.inputRDDs(basicPhysicalOperators.scala:50)\n\tat org.apache.spark.sql.execution.FilterExec.inputRDDs(basicPhysicalOperators.scala:149)\n\tat org.apache.spark.sql.execution.WholeStageCodegenExec.doExecute(WholeStageCodegenExec.scala:746)\n\tat org.apache.spark.sql.execution.SparkPlan.$anonfun$execute$1(SparkPlan.scala:180)\n\tat org.apache.spark.sql.execution.SparkPlan.$anonfun$executeQuery$1(SparkPlan.scala:218)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n\tat org.apache.spark.sql.execution.SparkPlan.executeQuery(SparkPlan.scala:215)\n\tat org.apache.spark.sql.execution.SparkPlan.execute(SparkPlan.scala:176)\n\tat org.apache.spark.sql.execution.columnar.CachedRDDBuilder.buildBuffers(InMemoryRelation.scala:252)\n\tat org.apache.spark.sql.execution.columnar.CachedRDDBuilder.cachedColumnBuffers(InMemoryRelation.scala:221)\n\tat org.apache.spark.sql.execution.columnar.InMemoryTableScanExec.filteredCachedBatches(InMemoryTableScanExec.scala:144)\n\tat org.apache.spark.sql.execution.columnar.InMemoryTableScanExec.inputRDD$lzycompute(InMemoryTableScanExec.scala:95)\n\tat org.apache.spark.sql.execution.columnar.InMemoryTableScanExec.inputRDD(InMemoryTableScanExec.scala:81)\n\tat org.apache.spark.sql.execution.columnar.InMemoryTableScanExec.doExecute(InMemoryTableScanExec.scala:155)\n\tat org.apache.spark.sql.execution.SparkPlan.$anonfun$execute$1(SparkPlan.scala:180)\n\tat org.apache.spark.sql.execution.SparkPlan.$anonfun$executeQuery$1(SparkPlan.scala:218)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n\tat org.apache.spark.sql.execution.SparkPlan.executeQuery(SparkPlan.scala:215)\n\tat org.apache.spark.sql.execution.SparkPlan.execute(SparkPlan.scala:176)\n\tat org.apache.spark.sql.execution.InputAdapter.inputRDD(WholeStageCodegenExec.scala:525)\n\tat org.apache.spark.sql.execution.InputRDDCodegen.inputRDDs(WholeStageCodegenExec.scala:453)\n\tat org.apache.spark.sql.execution.InputRDDCodegen.inputRDDs$(WholeStageCodegenExec.scala:452)\n\tat org.apache.spark.sql.execution.InputAdapter.inputRDDs(WholeStageCodegenExec.scala:496)\n\tat org.apache.spark.sql.execution.ProjectExec.inputRDDs(basicPhysicalOperators.scala:50)\n\tat org.apache.spark.sql.execution.WholeStageCodegenExec.doExecute(WholeStageCodegenExec.scala:746)\n\tat org.apache.spark.sql.execution.SparkPlan.$anonfun$execute$1(SparkPlan.scala:180)\n\tat org.apache.spark.sql.execution.SparkPlan.$anonfun$executeQuery$1(SparkPlan.scala:218)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n\tat org.apache.spark.sql.execution.SparkPlan.executeQuery(SparkPlan.scala:215)\n\tat org.apache.spark.sql.execution.SparkPlan.execute(SparkPlan.scala:176)\n\tat org.apache.spark.sql.execution.DeserializeToObjectExec.doExecute(objects.scala:96)\n\tat org.apache.spark.sql.execution.SparkPlan.$anonfun$execute$1(SparkPlan.scala:180)\n\tat org.apache.spark.sql.execution.SparkPlan.$anonfun$executeQuery$1(SparkPlan.scala:218)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n\tat org.apache.spark.sql.execution.SparkPlan.executeQuery(SparkPlan.scala:215)\n\tat org.apache.spark.sql.execution.SparkPlan.execute(SparkPlan.scala:176)\n\tat org.apache.spark.sql.execution.QueryExecution.toRdd$lzycompute(QueryExecution.scala:132)\n\tat org.apache.spark.sql.execution.QueryExecution.toRdd(QueryExecution.scala:131)\n\tat org.apache.spark.sql.Dataset.rdd$lzycompute(Dataset.scala:3241)\n\tat org.apache.spark.sql.Dataset.rdd(Dataset.scala:3239)\n\tat org.apache.spark.ml.util.Instrumentation.logDataset(Instrumentation.scala:62)\n\tat org.apache.spark.ml.classification.LinearSVC.$anonfun$train$1(LinearSVC.scala:173)\n\tat org.apache.spark.ml.util.Instrumentation$.$anonfun$instrumented$1(Instrumentation.scala:191)\n\tat scala.util.Try$.apply(Try.scala:213)\n\tat org.apache.spark.ml.util.Instrumentation$.instrumented(Instrumentation.scala:191)\n\tat org.apache.spark.ml.classification.LinearSVC.train(LinearSVC.scala:171)\n\tat org.apache.spark.ml.classification.LinearSVC.train(LinearSVC.scala:76)\n\tat org.apache.spark.ml.Predictor.fit(Predictor.scala:151)\n\tat org.apache.spark.ml.Predictor.fit(Predictor.scala:115)\n\tat jdk.internal.reflect.GeneratedMethodAccessor184.invoke(Unknown Source)\n\tat java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.base/java.lang.reflect.Method.invoke(Method.java:566)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\n\tat py4j.Gateway.invoke(Gateway.java:282)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.GatewayConnection.run(GatewayConnection.java:238)\n\tat java.base/java.lang.Thread.run(Thread.java:829)\nCaused by: org.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 33556.0 failed 1 times, most recent failure: Lost task 0.0 in stage 33556.0 (TID 2253059) (192.168.1.51 executor driver): TaskResultLost (result lost from block manager)\nDriver stacktrace:\n\tat org.apache.spark.scheduler.DAGScheduler.failJobAndIndependentStages(DAGScheduler.scala:2253)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2(DAGScheduler.scala:2202)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2$adapted(DAGScheduler.scala:2201)\n\tat scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62)\n\tat scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55)\n\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49)\n\tat org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:2201)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1(DAGScheduler.scala:1078)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1$adapted(DAGScheduler.scala:1078)\n\tat scala.Option.foreach(Option.scala:407)\n\tat org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:1078)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:2440)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2382)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2371)\n\tat org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49)\n\tat org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:868)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2202)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2223)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2242)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2267)\n\tat org.apache.spark.rdd.RDD.$anonfun$collect$1(RDD.scala:1030)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)\n\tat org.apache.spark.rdd.RDD.withScope(RDD.scala:414)\n\tat org.apache.spark.rdd.RDD.collect(RDD.scala:1029)\n\tat org.apache.spark.sql.execution.SparkPlan.executeCollectIterator(SparkPlan.scala:397)\n\tat org.apache.spark.sql.execution.exchange.BroadcastExchangeExec.$anonfun$relationFuture$1(BroadcastExchangeExec.scala:118)\n\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withThreadLocalCaptured$1(SQLExecution.scala:185)\n\tat java.base/java.util.concurrent.FutureTask.run(FutureTask.java:264)\n\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\n\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\n\t... 1 more\n",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mPy4JJavaError\u001B[0m                             Traceback (most recent call last)",
      "\u001B[0;32m<ipython-input-25-9a03e2eea1f2>\u001B[0m in \u001B[0;36m<module>\u001B[0;34m\u001B[0m\n\u001B[1;32m      1\u001B[0m \u001B[0;31m#grid_search, hyperpar tuning...\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m----> 2\u001B[0;31m \u001B[0mestimator\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mcvs\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mfit\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mtraining_set\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m      3\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m      4\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m~/.local/lib/python3.8/site-packages/pyspark/ml/base.py\u001B[0m in \u001B[0;36mfit\u001B[0;34m(self, dataset, params)\u001B[0m\n\u001B[1;32m    159\u001B[0m                 \u001B[0;32mreturn\u001B[0m \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mcopy\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mparams\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0m_fit\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mdataset\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    160\u001B[0m             \u001B[0;32melse\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m--> 161\u001B[0;31m                 \u001B[0;32mreturn\u001B[0m \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0m_fit\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mdataset\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m    162\u001B[0m         \u001B[0;32melse\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    163\u001B[0m             raise ValueError(\"Params must be either a param map or a list/tuple of param maps, \"\n",
      "\u001B[0;32m~/.local/lib/python3.8/site-packages/pyspark/ml/tuning.py\u001B[0m in \u001B[0;36m_fit\u001B[0;34m(self, dataset)\u001B[0m\n\u001B[1;32m    685\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    686\u001B[0m             \u001B[0mtasks\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0m_parallelFitTasks\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mest\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mtrain\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0meva\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mvalidation\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mepm\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mcollectSubModelsParam\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m--> 687\u001B[0;31m             \u001B[0;32mfor\u001B[0m \u001B[0mj\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mmetric\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0msubModel\u001B[0m \u001B[0;32min\u001B[0m \u001B[0mpool\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mimap_unordered\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;32mlambda\u001B[0m \u001B[0mf\u001B[0m\u001B[0;34m:\u001B[0m \u001B[0mf\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mtasks\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m    688\u001B[0m                 \u001B[0mmetrics\u001B[0m\u001B[0;34m[\u001B[0m\u001B[0mj\u001B[0m\u001B[0;34m]\u001B[0m \u001B[0;34m+=\u001B[0m \u001B[0;34m(\u001B[0m\u001B[0mmetric\u001B[0m \u001B[0;34m/\u001B[0m \u001B[0mnFolds\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    689\u001B[0m                 \u001B[0;32mif\u001B[0m \u001B[0mcollectSubModelsParam\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m/usr/lib/python3.8/multiprocessing/pool.py\u001B[0m in \u001B[0;36mnext\u001B[0;34m(self, timeout)\u001B[0m\n\u001B[1;32m    866\u001B[0m         \u001B[0;32mif\u001B[0m \u001B[0msuccess\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    867\u001B[0m             \u001B[0;32mreturn\u001B[0m \u001B[0mvalue\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m--> 868\u001B[0;31m         \u001B[0;32mraise\u001B[0m \u001B[0mvalue\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m    869\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    870\u001B[0m     \u001B[0m__next__\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mnext\u001B[0m                    \u001B[0;31m# XXX\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m/usr/lib/python3.8/multiprocessing/pool.py\u001B[0m in \u001B[0;36mworker\u001B[0;34m(inqueue, outqueue, initializer, initargs, maxtasks, wrap_exception)\u001B[0m\n\u001B[1;32m    123\u001B[0m         \u001B[0mjob\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mi\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mfunc\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0margs\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mkwds\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mtask\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    124\u001B[0m         \u001B[0;32mtry\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m--> 125\u001B[0;31m             \u001B[0mresult\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0;34m(\u001B[0m\u001B[0;32mTrue\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mfunc\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m*\u001B[0m\u001B[0margs\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0;34m**\u001B[0m\u001B[0mkwds\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m    126\u001B[0m         \u001B[0;32mexcept\u001B[0m \u001B[0mException\u001B[0m \u001B[0;32mas\u001B[0m \u001B[0me\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    127\u001B[0m             \u001B[0;32mif\u001B[0m \u001B[0mwrap_exception\u001B[0m \u001B[0;32mand\u001B[0m \u001B[0mfunc\u001B[0m \u001B[0;32mis\u001B[0m \u001B[0;32mnot\u001B[0m \u001B[0m_helper_reraises_exception\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m~/.local/lib/python3.8/site-packages/pyspark/ml/tuning.py\u001B[0m in \u001B[0;36m<lambda>\u001B[0;34m(f)\u001B[0m\n\u001B[1;32m    685\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    686\u001B[0m             \u001B[0mtasks\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0m_parallelFitTasks\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mest\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mtrain\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0meva\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mvalidation\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mepm\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mcollectSubModelsParam\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m--> 687\u001B[0;31m             \u001B[0;32mfor\u001B[0m \u001B[0mj\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mmetric\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0msubModel\u001B[0m \u001B[0;32min\u001B[0m \u001B[0mpool\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mimap_unordered\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;32mlambda\u001B[0m \u001B[0mf\u001B[0m\u001B[0;34m:\u001B[0m \u001B[0mf\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mtasks\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m    688\u001B[0m                 \u001B[0mmetrics\u001B[0m\u001B[0;34m[\u001B[0m\u001B[0mj\u001B[0m\u001B[0;34m]\u001B[0m \u001B[0;34m+=\u001B[0m \u001B[0;34m(\u001B[0m\u001B[0mmetric\u001B[0m \u001B[0;34m/\u001B[0m \u001B[0mnFolds\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    689\u001B[0m                 \u001B[0;32mif\u001B[0m \u001B[0mcollectSubModelsParam\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m~/.local/lib/python3.8/site-packages/pyspark/ml/tuning.py\u001B[0m in \u001B[0;36msingleTask\u001B[0;34m()\u001B[0m\n\u001B[1;32m     67\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m     68\u001B[0m     \u001B[0;32mdef\u001B[0m \u001B[0msingleTask\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m---> 69\u001B[0;31m         \u001B[0mindex\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mmodel\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mnext\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mmodelIter\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m     70\u001B[0m         \u001B[0;31m# TODO: duplicate evaluator to take extra params from input\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m     71\u001B[0m         \u001B[0;31m#  Note: Supporting tuning params in evaluator need update method\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m~/.local/lib/python3.8/site-packages/pyspark/ml/base.py\u001B[0m in \u001B[0;36m__next__\u001B[0;34m(self)\u001B[0m\n\u001B[1;32m     67\u001B[0m                 \u001B[0;32mraise\u001B[0m \u001B[0mStopIteration\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m\"No models remaining.\"\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m     68\u001B[0m             \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mcounter\u001B[0m \u001B[0;34m+=\u001B[0m \u001B[0;36m1\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m---> 69\u001B[0;31m         \u001B[0;32mreturn\u001B[0m \u001B[0mindex\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mfitSingleModel\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mindex\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m     70\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m     71\u001B[0m     \u001B[0;32mdef\u001B[0m \u001B[0mnext\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mself\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m~/.local/lib/python3.8/site-packages/pyspark/ml/base.py\u001B[0m in \u001B[0;36mfitSingleModel\u001B[0;34m(index)\u001B[0m\n\u001B[1;32m    124\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    125\u001B[0m         \u001B[0;32mdef\u001B[0m \u001B[0mfitSingleModel\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mindex\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m--> 126\u001B[0;31m             \u001B[0;32mreturn\u001B[0m \u001B[0mestimator\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mfit\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mdataset\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mparamMaps\u001B[0m\u001B[0;34m[\u001B[0m\u001B[0mindex\u001B[0m\u001B[0;34m]\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m    127\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    128\u001B[0m         \u001B[0;32mreturn\u001B[0m \u001B[0m_FitMultipleIterator\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mfitSingleModel\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mlen\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mparamMaps\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m~/.local/lib/python3.8/site-packages/pyspark/ml/base.py\u001B[0m in \u001B[0;36mfit\u001B[0;34m(self, dataset, params)\u001B[0m\n\u001B[1;32m    157\u001B[0m         \u001B[0;32melif\u001B[0m \u001B[0misinstance\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mparams\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mdict\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    158\u001B[0m             \u001B[0;32mif\u001B[0m \u001B[0mparams\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m--> 159\u001B[0;31m                 \u001B[0;32mreturn\u001B[0m \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mcopy\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mparams\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0m_fit\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mdataset\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m    160\u001B[0m             \u001B[0;32melse\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    161\u001B[0m                 \u001B[0;32mreturn\u001B[0m \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0m_fit\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mdataset\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m~/.local/lib/python3.8/site-packages/pyspark/ml/wrapper.py\u001B[0m in \u001B[0;36m_fit\u001B[0;34m(self, dataset)\u001B[0m\n\u001B[1;32m    333\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    334\u001B[0m     \u001B[0;32mdef\u001B[0m \u001B[0m_fit\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mself\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mdataset\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m--> 335\u001B[0;31m         \u001B[0mjava_model\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0m_fit_java\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mdataset\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m    336\u001B[0m         \u001B[0mmodel\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0m_create_model\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mjava_model\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    337\u001B[0m         \u001B[0;32mreturn\u001B[0m \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0m_copyValues\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mmodel\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m~/.local/lib/python3.8/site-packages/pyspark/ml/wrapper.py\u001B[0m in \u001B[0;36m_fit_java\u001B[0;34m(self, dataset)\u001B[0m\n\u001B[1;32m    330\u001B[0m         \"\"\"\n\u001B[1;32m    331\u001B[0m         \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0m_transfer_params_to_java\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m--> 332\u001B[0;31m         \u001B[0;32mreturn\u001B[0m \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0m_java_obj\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mfit\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mdataset\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0m_jdf\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m    333\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    334\u001B[0m     \u001B[0;32mdef\u001B[0m \u001B[0m_fit\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mself\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mdataset\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m~/.local/lib/python3.8/site-packages/py4j/java_gateway.py\u001B[0m in \u001B[0;36m__call__\u001B[0;34m(self, *args)\u001B[0m\n\u001B[1;32m   1307\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m   1308\u001B[0m         \u001B[0manswer\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mgateway_client\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0msend_command\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mcommand\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m-> 1309\u001B[0;31m         return_value = get_return_value(\n\u001B[0m\u001B[1;32m   1310\u001B[0m             answer, self.gateway_client, self.target_id, self.name)\n\u001B[1;32m   1311\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m~/.local/lib/python3.8/site-packages/pyspark/sql/utils.py\u001B[0m in \u001B[0;36mdeco\u001B[0;34m(*a, **kw)\u001B[0m\n\u001B[1;32m    109\u001B[0m     \u001B[0;32mdef\u001B[0m \u001B[0mdeco\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m*\u001B[0m\u001B[0ma\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0;34m**\u001B[0m\u001B[0mkw\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    110\u001B[0m         \u001B[0;32mtry\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m--> 111\u001B[0;31m             \u001B[0;32mreturn\u001B[0m \u001B[0mf\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m*\u001B[0m\u001B[0ma\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0;34m**\u001B[0m\u001B[0mkw\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m    112\u001B[0m         \u001B[0;32mexcept\u001B[0m \u001B[0mpy4j\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mprotocol\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mPy4JJavaError\u001B[0m \u001B[0;32mas\u001B[0m \u001B[0me\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    113\u001B[0m             \u001B[0mconverted\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mconvert_exception\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0me\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mjava_exception\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m~/.local/lib/python3.8/site-packages/py4j/protocol.py\u001B[0m in \u001B[0;36mget_return_value\u001B[0;34m(answer, gateway_client, target_id, name)\u001B[0m\n\u001B[1;32m    324\u001B[0m             \u001B[0mvalue\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mOUTPUT_CONVERTER\u001B[0m\u001B[0;34m[\u001B[0m\u001B[0mtype\u001B[0m\u001B[0;34m]\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0manswer\u001B[0m\u001B[0;34m[\u001B[0m\u001B[0;36m2\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m]\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mgateway_client\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    325\u001B[0m             \u001B[0;32mif\u001B[0m \u001B[0manswer\u001B[0m\u001B[0;34m[\u001B[0m\u001B[0;36m1\u001B[0m\u001B[0;34m]\u001B[0m \u001B[0;34m==\u001B[0m \u001B[0mREFERENCE_TYPE\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m--> 326\u001B[0;31m                 raise Py4JJavaError(\n\u001B[0m\u001B[1;32m    327\u001B[0m                     \u001B[0;34m\"An error occurred while calling {0}{1}{2}.\\n\"\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    328\u001B[0m                     format(target_id, \".\", name), value)\n",
      "\u001B[0;31mPy4JJavaError\u001B[0m: An error occurred while calling o7215.fit.\n: java.util.concurrent.ExecutionException: org.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 33556.0 failed 1 times, most recent failure: Lost task 0.0 in stage 33556.0 (TID 2253059) (192.168.1.51 executor driver): TaskResultLost (result lost from block manager)\nDriver stacktrace:\n\tat java.base/java.util.concurrent.FutureTask.report(FutureTask.java:122)\n\tat java.base/java.util.concurrent.FutureTask.get(FutureTask.java:205)\n\tat org.apache.spark.sql.execution.exchange.BroadcastExchangeExec.doExecuteBroadcast(BroadcastExchangeExec.scala:194)\n\tat org.apache.spark.sql.execution.InputAdapter.doExecuteBroadcast(WholeStageCodegenExec.scala:515)\n\tat org.apache.spark.sql.execution.SparkPlan.$anonfun$executeBroadcast$1(SparkPlan.scala:193)\n\tat org.apache.spark.sql.execution.SparkPlan.$anonfun$executeQuery$1(SparkPlan.scala:218)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n\tat org.apache.spark.sql.execution.SparkPlan.executeQuery(SparkPlan.scala:215)\n\tat org.apache.spark.sql.execution.SparkPlan.executeBroadcast(SparkPlan.scala:189)\n\tat org.apache.spark.sql.execution.joins.BroadcastHashJoinExec.prepareBroadcast(BroadcastHashJoinExec.scala:203)\n\tat org.apache.spark.sql.execution.joins.BroadcastHashJoinExec.prepareRelation(BroadcastHashJoinExec.scala:217)\n\tat org.apache.spark.sql.execution.joins.HashJoin.codegenInner(HashJoin.scala:443)\n\tat org.apache.spark.sql.execution.joins.HashJoin.codegenInner$(HashJoin.scala:442)\n\tat org.apache.spark.sql.execution.joins.BroadcastHashJoinExec.codegenInner(BroadcastHashJoinExec.scala:40)\n\tat org.apache.spark.sql.execution.joins.HashJoin.doConsume(HashJoin.scala:351)\n\tat org.apache.spark.sql.execution.joins.HashJoin.doConsume$(HashJoin.scala:349)\n\tat org.apache.spark.sql.execution.joins.BroadcastHashJoinExec.doConsume(BroadcastHashJoinExec.scala:40)\n\tat org.apache.spark.sql.execution.CodegenSupport.consume(WholeStageCodegenExec.scala:194)\n\tat org.apache.spark.sql.execution.CodegenSupport.consume$(WholeStageCodegenExec.scala:149)\n\tat org.apache.spark.sql.execution.InputAdapter.consume(WholeStageCodegenExec.scala:496)\n\tat org.apache.spark.sql.execution.InputRDDCodegen.doProduce(WholeStageCodegenExec.scala:483)\n\tat org.apache.spark.sql.execution.InputRDDCodegen.doProduce$(WholeStageCodegenExec.scala:456)\n\tat org.apache.spark.sql.execution.InputAdapter.doProduce(WholeStageCodegenExec.scala:496)\n\tat org.apache.spark.sql.execution.CodegenSupport.$anonfun$produce$1(WholeStageCodegenExec.scala:95)\n\tat org.apache.spark.sql.execution.SparkPlan.$anonfun$executeQuery$1(SparkPlan.scala:218)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n\tat org.apache.spark.sql.execution.SparkPlan.executeQuery(SparkPlan.scala:215)\n\tat org.apache.spark.sql.execution.CodegenSupport.produce(WholeStageCodegenExec.scala:90)\n\tat org.apache.spark.sql.execution.CodegenSupport.produce$(WholeStageCodegenExec.scala:90)\n\tat org.apache.spark.sql.execution.InputAdapter.produce(WholeStageCodegenExec.scala:496)\n\tat org.apache.spark.sql.execution.joins.HashJoin.doProduce(HashJoin.scala:346)\n\tat org.apache.spark.sql.execution.joins.HashJoin.doProduce$(HashJoin.scala:345)\n\tat org.apache.spark.sql.execution.joins.BroadcastHashJoinExec.doProduce(BroadcastHashJoinExec.scala:40)\n\tat org.apache.spark.sql.execution.CodegenSupport.$anonfun$produce$1(WholeStageCodegenExec.scala:95)\n\tat org.apache.spark.sql.execution.SparkPlan.$anonfun$executeQuery$1(SparkPlan.scala:218)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n\tat org.apache.spark.sql.execution.SparkPlan.executeQuery(SparkPlan.scala:215)\n\tat org.apache.spark.sql.execution.CodegenSupport.produce(WholeStageCodegenExec.scala:90)\n\tat org.apache.spark.sql.execution.CodegenSupport.produce$(WholeStageCodegenExec.scala:90)\n\tat org.apache.spark.sql.execution.joins.BroadcastHashJoinExec.produce(BroadcastHashJoinExec.scala:40)\n\tat org.apache.spark.sql.execution.ProjectExec.doProduce(basicPhysicalOperators.scala:54)\n\tat org.apache.spark.sql.execution.CodegenSupport.$anonfun$produce$1(WholeStageCodegenExec.scala:95)\n\tat org.apache.spark.sql.execution.SparkPlan.$anonfun$executeQuery$1(SparkPlan.scala:218)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n\tat org.apache.spark.sql.execution.SparkPlan.executeQuery(SparkPlan.scala:215)\n\tat org.apache.spark.sql.execution.CodegenSupport.produce(WholeStageCodegenExec.scala:90)\n\tat org.apache.spark.sql.execution.CodegenSupport.produce$(WholeStageCodegenExec.scala:90)\n\tat org.apache.spark.sql.execution.ProjectExec.produce(basicPhysicalOperators.scala:41)\n\tat org.apache.spark.sql.execution.joins.HashJoin.doProduce(HashJoin.scala:346)\n\tat org.apache.spark.sql.execution.joins.HashJoin.doProduce$(HashJoin.scala:345)\n\tat org.apache.spark.sql.execution.joins.BroadcastHashJoinExec.doProduce(BroadcastHashJoinExec.scala:40)\n\tat org.apache.spark.sql.execution.CodegenSupport.$anonfun$produce$1(WholeStageCodegenExec.scala:95)\n\tat org.apache.spark.sql.execution.SparkPlan.$anonfun$executeQuery$1(SparkPlan.scala:218)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n\tat org.apache.spark.sql.execution.SparkPlan.executeQuery(SparkPlan.scala:215)\n\tat org.apache.spark.sql.execution.CodegenSupport.produce(WholeStageCodegenExec.scala:90)\n\tat org.apache.spark.sql.execution.CodegenSupport.produce$(WholeStageCodegenExec.scala:90)\n\tat org.apache.spark.sql.execution.joins.BroadcastHashJoinExec.produce(BroadcastHashJoinExec.scala:40)\n\tat org.apache.spark.sql.execution.ProjectExec.doProduce(basicPhysicalOperators.scala:54)\n\tat org.apache.spark.sql.execution.CodegenSupport.$anonfun$produce$1(WholeStageCodegenExec.scala:95)\n\tat org.apache.spark.sql.execution.SparkPlan.$anonfun$executeQuery$1(SparkPlan.scala:218)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n\tat org.apache.spark.sql.execution.SparkPlan.executeQuery(SparkPlan.scala:215)\n\tat org.apache.spark.sql.execution.CodegenSupport.produce(WholeStageCodegenExec.scala:90)\n\tat org.apache.spark.sql.execution.CodegenSupport.produce$(WholeStageCodegenExec.scala:90)\n\tat org.apache.spark.sql.execution.ProjectExec.produce(basicPhysicalOperators.scala:41)\n\tat org.apache.spark.sql.execution.WholeStageCodegenExec.doCodeGen(WholeStageCodegenExec.scala:655)\n\tat org.apache.spark.sql.execution.WholeStageCodegenExec.doExecute(WholeStageCodegenExec.scala:718)\n\tat org.apache.spark.sql.execution.SparkPlan.$anonfun$execute$1(SparkPlan.scala:180)\n\tat org.apache.spark.sql.execution.SparkPlan.$anonfun$executeQuery$1(SparkPlan.scala:218)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n\tat org.apache.spark.sql.execution.SparkPlan.executeQuery(SparkPlan.scala:215)\n\tat org.apache.spark.sql.execution.SparkPlan.execute(SparkPlan.scala:176)\n\tat org.apache.spark.sql.execution.python.EvalPythonExec.doExecute(EvalPythonExec.scala:88)\n\tat org.apache.spark.sql.execution.python.EvalPythonExec.doExecute$(EvalPythonExec.scala:87)\n\tat org.apache.spark.sql.execution.python.BatchEvalPythonExec.doExecute(BatchEvalPythonExec.scala:34)\n\tat org.apache.spark.sql.execution.SparkPlan.$anonfun$execute$1(SparkPlan.scala:180)\n\tat org.apache.spark.sql.execution.SparkPlan.$anonfun$executeQuery$1(SparkPlan.scala:218)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n\tat org.apache.spark.sql.execution.SparkPlan.executeQuery(SparkPlan.scala:215)\n\tat org.apache.spark.sql.execution.SparkPlan.execute(SparkPlan.scala:176)\n\tat org.apache.spark.sql.execution.InputAdapter.inputRDD(WholeStageCodegenExec.scala:525)\n\tat org.apache.spark.sql.execution.InputRDDCodegen.inputRDDs(WholeStageCodegenExec.scala:453)\n\tat org.apache.spark.sql.execution.InputRDDCodegen.inputRDDs$(WholeStageCodegenExec.scala:452)\n\tat org.apache.spark.sql.execution.InputAdapter.inputRDDs(WholeStageCodegenExec.scala:496)\n\tat org.apache.spark.sql.execution.ProjectExec.inputRDDs(basicPhysicalOperators.scala:50)\n\tat org.apache.spark.sql.execution.SortExec.inputRDDs(SortExec.scala:132)\n\tat org.apache.spark.sql.execution.SampleExec.inputRDDs(basicPhysicalOperators.scala:311)\n\tat org.apache.spark.sql.execution.ProjectExec.inputRDDs(basicPhysicalOperators.scala:50)\n\tat org.apache.spark.sql.execution.FilterExec.inputRDDs(basicPhysicalOperators.scala:149)\n\tat org.apache.spark.sql.execution.WholeStageCodegenExec.doExecute(WholeStageCodegenExec.scala:746)\n\tat org.apache.spark.sql.execution.SparkPlan.$anonfun$execute$1(SparkPlan.scala:180)\n\tat org.apache.spark.sql.execution.SparkPlan.$anonfun$executeQuery$1(SparkPlan.scala:218)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n\tat org.apache.spark.sql.execution.SparkPlan.executeQuery(SparkPlan.scala:215)\n\tat org.apache.spark.sql.execution.SparkPlan.execute(SparkPlan.scala:176)\n\tat org.apache.spark.sql.execution.columnar.CachedRDDBuilder.buildBuffers(InMemoryRelation.scala:252)\n\tat org.apache.spark.sql.execution.columnar.CachedRDDBuilder.cachedColumnBuffers(InMemoryRelation.scala:221)\n\tat org.apache.spark.sql.execution.columnar.InMemoryTableScanExec.filteredCachedBatches(InMemoryTableScanExec.scala:144)\n\tat org.apache.spark.sql.execution.columnar.InMemoryTableScanExec.inputRDD$lzycompute(InMemoryTableScanExec.scala:95)\n\tat org.apache.spark.sql.execution.columnar.InMemoryTableScanExec.inputRDD(InMemoryTableScanExec.scala:81)\n\tat org.apache.spark.sql.execution.columnar.InMemoryTableScanExec.doExecute(InMemoryTableScanExec.scala:155)\n\tat org.apache.spark.sql.execution.SparkPlan.$anonfun$execute$1(SparkPlan.scala:180)\n\tat org.apache.spark.sql.execution.SparkPlan.$anonfun$executeQuery$1(SparkPlan.scala:218)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n\tat org.apache.spark.sql.execution.SparkPlan.executeQuery(SparkPlan.scala:215)\n\tat org.apache.spark.sql.execution.SparkPlan.execute(SparkPlan.scala:176)\n\tat org.apache.spark.sql.execution.InputAdapter.inputRDD(WholeStageCodegenExec.scala:525)\n\tat org.apache.spark.sql.execution.InputRDDCodegen.inputRDDs(WholeStageCodegenExec.scala:453)\n\tat org.apache.spark.sql.execution.InputRDDCodegen.inputRDDs$(WholeStageCodegenExec.scala:452)\n\tat org.apache.spark.sql.execution.InputAdapter.inputRDDs(WholeStageCodegenExec.scala:496)\n\tat org.apache.spark.sql.execution.ProjectExec.inputRDDs(basicPhysicalOperators.scala:50)\n\tat org.apache.spark.sql.execution.WholeStageCodegenExec.doExecute(WholeStageCodegenExec.scala:746)\n\tat org.apache.spark.sql.execution.SparkPlan.$anonfun$execute$1(SparkPlan.scala:180)\n\tat org.apache.spark.sql.execution.SparkPlan.$anonfun$executeQuery$1(SparkPlan.scala:218)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n\tat org.apache.spark.sql.execution.SparkPlan.executeQuery(SparkPlan.scala:215)\n\tat org.apache.spark.sql.execution.SparkPlan.execute(SparkPlan.scala:176)\n\tat org.apache.spark.sql.execution.DeserializeToObjectExec.doExecute(objects.scala:96)\n\tat org.apache.spark.sql.execution.SparkPlan.$anonfun$execute$1(SparkPlan.scala:180)\n\tat org.apache.spark.sql.execution.SparkPlan.$anonfun$executeQuery$1(SparkPlan.scala:218)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n\tat org.apache.spark.sql.execution.SparkPlan.executeQuery(SparkPlan.scala:215)\n\tat org.apache.spark.sql.execution.SparkPlan.execute(SparkPlan.scala:176)\n\tat org.apache.spark.sql.execution.QueryExecution.toRdd$lzycompute(QueryExecution.scala:132)\n\tat org.apache.spark.sql.execution.QueryExecution.toRdd(QueryExecution.scala:131)\n\tat org.apache.spark.sql.Dataset.rdd$lzycompute(Dataset.scala:3241)\n\tat org.apache.spark.sql.Dataset.rdd(Dataset.scala:3239)\n\tat org.apache.spark.ml.util.Instrumentation.logDataset(Instrumentation.scala:62)\n\tat org.apache.spark.ml.classification.LinearSVC.$anonfun$train$1(LinearSVC.scala:173)\n\tat org.apache.spark.ml.util.Instrumentation$.$anonfun$instrumented$1(Instrumentation.scala:191)\n\tat scala.util.Try$.apply(Try.scala:213)\n\tat org.apache.spark.ml.util.Instrumentation$.instrumented(Instrumentation.scala:191)\n\tat org.apache.spark.ml.classification.LinearSVC.train(LinearSVC.scala:171)\n\tat org.apache.spark.ml.classification.LinearSVC.train(LinearSVC.scala:76)\n\tat org.apache.spark.ml.Predictor.fit(Predictor.scala:151)\n\tat org.apache.spark.ml.Predictor.fit(Predictor.scala:115)\n\tat jdk.internal.reflect.GeneratedMethodAccessor184.invoke(Unknown Source)\n\tat java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.base/java.lang.reflect.Method.invoke(Method.java:566)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\n\tat py4j.Gateway.invoke(Gateway.java:282)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.GatewayConnection.run(GatewayConnection.java:238)\n\tat java.base/java.lang.Thread.run(Thread.java:829)\nCaused by: org.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 33556.0 failed 1 times, most recent failure: Lost task 0.0 in stage 33556.0 (TID 2253059) (192.168.1.51 executor driver): TaskResultLost (result lost from block manager)\nDriver stacktrace:\n\tat org.apache.spark.scheduler.DAGScheduler.failJobAndIndependentStages(DAGScheduler.scala:2253)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2(DAGScheduler.scala:2202)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2$adapted(DAGScheduler.scala:2201)\n\tat scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62)\n\tat scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55)\n\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49)\n\tat org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:2201)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1(DAGScheduler.scala:1078)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1$adapted(DAGScheduler.scala:1078)\n\tat scala.Option.foreach(Option.scala:407)\n\tat org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:1078)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:2440)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2382)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2371)\n\tat org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49)\n\tat org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:868)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2202)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2223)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2242)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2267)\n\tat org.apache.spark.rdd.RDD.$anonfun$collect$1(RDD.scala:1030)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)\n\tat org.apache.spark.rdd.RDD.withScope(RDD.scala:414)\n\tat org.apache.spark.rdd.RDD.collect(RDD.scala:1029)\n\tat org.apache.spark.sql.execution.SparkPlan.executeCollectIterator(SparkPlan.scala:397)\n\tat org.apache.spark.sql.execution.exchange.BroadcastExchangeExec.$anonfun$relationFuture$1(BroadcastExchangeExec.scala:118)\n\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withThreadLocalCaptured$1(SQLExecution.scala:185)\n\tat java.base/java.util.concurrent.FutureTask.run(FutureTask.java:264)\n\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\n\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\n\t... 1 more\n"
     ]
    }
   ],
   "source": [
    "#grid_search, hyperpar tuning...\n",
    "estimator = cvs.fit(training_set)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "prediction = estimator.transform(test_set).select('lid','rid','label','prediction')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "accuracy = prediction.filter(f.col('label')==f.col('prediction').cast(t.IntegerType())).count() / prediction.count()\n",
    "print(\"Accuracy: \", accuracy)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}